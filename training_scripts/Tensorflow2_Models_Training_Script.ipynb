{"cells":[{"cell_type":"markdown","metadata":{"id":"fF8ysCfYKgTP"},"source":["# TensorFlow Lite Object Detection API in Colab\n","\n","\n"]},{"cell_type":"markdown","source":["#1.&nbsp; Introduction\n","\n","This notebook uses [the TensorFlow 2 Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to train an SSD-MobileNet model or EfficientDet model with a custom dataset and convert it to TensorFlow Lite format. By working through this Colab, you'll be able to create and download a TFLite model that you can run on your PC, an Android phone, or an edge device like the Raspberry Pi.\n","\n","\n","### Working in Colab\n","Colab provides a virtual machine in your browser complete with a Linux OS, filesystem, Python environment, and best of all, a free GPU. It comes with most TensorFlow backend requirements (like CUDA and cuDNN) pre-installed. Simply click the play button on sections of code in this notebook to execute them on the virtual machine.\n","\n","> *Note: Make sure you're using a GPU-equipped machine by going to \"Runtime\" -> \"Change runtime type\" in the top menu bar, and then selecting \"GPU\" from the Hardware accelerator dropdown.*\n","\n","\n","### *Navigation*\n","This is a long notebook! Each step of the training process has its own section. Click the arrow next to the heading for each section to expand it. You can use the table of contents in the left sidebar to jump from section to section."],"metadata":{"id":"1JafuPg9gC5m"}},{"cell_type":"markdown","metadata":{"id":"sxb8_h-QFErO"},"source":["#2.&nbsp;Install TensorFlow Object Detection Dependencies"]},{"cell_type":"markdown","metadata":{"id":"l7EOtpvlLeS0"},"source":["First, we'll install the TensorFlow Object Detection API in this Google Colab instance. This requires cloning the [TensorFlow models repository](https://github.com/tensorflow/models) and running a couple installation commands. Click the play button to run the following sections of code.\n","\n","The latest version of TensorFlow this Colab has been verified to work with is TF v2.8.0.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9067,"status":"ok","timestamp":1688428564120,"user":{"displayName":"Ahmed Boussihmed","userId":"16936910001719053027"},"user_tz":420},"id":"ypWGYdPlLRUN","outputId":"f6846d3a-9c07-4200-db26-56419578ae27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'models'...\n","remote: Enumerating objects: 3916, done.\u001b[K\n","remote: Counting objects: 100% (3916/3916), done.\u001b[K\n","remote: Compressing objects: 100% (3025/3025), done.\u001b[K\n","remote: Total 3916 (delta 1130), reused 1812 (delta 838), pack-reused 0\u001b[K\n","Receiving objects: 100% (3916/3916), 49.65 MiB | 14.48 MiB/s, done.\n","Resolving deltas: 100% (1130/1130), done.\n"]}],"source":["# Clone the tensorflow models repository from GitHub\n","!git clone --depth 1 https://github.com/tensorflow/models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QPmVBSlLTzM"},"outputs":[],"source":["# Copy setup files into models/research folder\n","%%bash\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","#cp object_detection/packages/tf2/setup.py ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRBnuCKjM4Bd"},"outputs":[],"source":["# Modify setup.py file to install the tf-models-official repository targeted at TF v2.8.0\n","import re\n","with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n","    s = f.read()\n","\n","with open('/content/models/research/setup.py', 'w') as f:\n","    # Set fine_tune_checkpoint path\n","    s = re.sub('tf-models-official>=2.5.1',\n","               'tf-models-official==2.8.0', s)\n","    f.write(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":209406,"status":"ok","timestamp":1688428805171,"user":{"displayName":"Ahmed Boussihmed","userId":"16936910001719053027"},"user_tz":420},"id":"OLDnCkLLwLr6","outputId":"f0b7dfe4-bda0-4e22-9ec0-a62640ff1e06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing ./models/research\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting avro-python3 (from object-detection==0.1)\n","  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting apache-beam (from object-detection==0.1)\n","  Downloading apache_beam-2.48.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (8.4.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (4.9.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.7.1)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (0.29.35)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (0.6.0.post1)\n","Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.16.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.0.6)\n","Collecting lvis (from object-detection==0.1)\n","  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.10.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.5.3)\n","Collecting tf-models-official==2.8.0 (from object-detection==0.1)\n","  Downloading tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow_io (from object-detection==0.1)\n","  Downloading tensorflow_io-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/28.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.12.0)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.4.7)\n","Collecting sacrebleu<=2.2.0 (from object-detection==0.1)\n","  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (0.5.0)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (2.84.0)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (1.5.13)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (1.22.4)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (4.1.3)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (4.7.0.72)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (5.9.5)\n","Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (9.0.0)\n","Collecting pyyaml<6.0,>=5.1 (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting sentencepiece (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorflow-addons (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (4.9.2)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (0.13.0)\n","Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-text~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorflow_text-2.8.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl (498.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2022.7.1)\n","Collecting portalocker (from sacrebleu<=2.2.0->object-detection==0.1)\n","  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2022.10.31)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.8.10)\n","Collecting colorama (from sacrebleu<=2.2.0->object-detection==0.1)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim->object-detection==0.1) (1.4.0)\n","Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\n","  Downloading crcmod-1.7.tar.gz (89 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting orjson<4.0 (from apache-beam->object-detection==0.1)\n","  Downloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.2.1)\n","Collecting fastavro<2,>=0.23.6 (from apache-beam->object-detection==0.1)\n","  Downloading fastavro-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam->object-detection==0.1)\n","  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n","Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.56.0)\n","Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\n","  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n","Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.21.0)\n","Collecting objsize<0.7.0,>=0.6.1 (from apache-beam->object-detection==0.1)\n","  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n","Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\n","  Downloading pymongo-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (648 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.9/648.9 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.22.3)\n","Requirement already satisfied: protobuf<4.24.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.20.3)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.4.2)\n","Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.27.1)\n","Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.6.3)\n","Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n","  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (4.7.0.72)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (1.1.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (4.40.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (23.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem==0.32.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io->object-detection==0.1) (0.32.0)\n","Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (2.17.3)\n","Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (0.1.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (2.11.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (4.1.1)\n","Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (2022.12.7)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (4.65.0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (8.0.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (1.26.16)\n","Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n","  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.8.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.3.0)\n","INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n","Collecting tensorflow~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorflow-2.8.3-cp310-cp310-manylinux2010_x86_64.whl (498.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.5/498.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.2-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.1-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (67.7.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (2.3.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.9,>=2.8 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras (from object-detection==0.1)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.8.0->object-detection==0.1) (0.1.8)\n","Collecting numpy>=1.15.4 (from tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object-detection==0.1) (0.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object-detection==0.1) (0.3.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object-detection==0.1) (4.9)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.8.0->object-detection==0.1) (1.2.2)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (0.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (8.1.3)\n","Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (1.3.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (2.3)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (1.13.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (0.10.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (0.40.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (5.12.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (3.15.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (1.59.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (5.3.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.8.0->object-detection==0.1) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.8.0->object-detection==0.1) (3.1.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.4.3)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (2.3.6)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (1.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (2.1.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.2.2)\n","Building wheels for collected packages: object-detection, avro-python3, crcmod, dill, pyyaml, seqeval, docopt\n","  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1696981 sha256=5e32a8d1b55af062652dc96d93049ae7136fc1fe869971b9a5c466c28c70f16a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2709r7_s/wheels/53/dd/70/2de274d6c443c69d367bd6a5606f95e5a6df61aacf1435ec0d\n","  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43994 sha256=b2530af97f4ceafd98f4665245016e0e4085141f745bbe65814032b742e9f035\n","  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n","  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=37094 sha256=c31798f9d8028255854366b0ba3cfab71150e0b1ebafaadcbed1e4835ae6452f\n","  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78545 sha256=abd14c75689211244751c62395e4de7065471ab65fce06ac4ec6e70bfadef587\n","  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n","  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=70bc9f5f86986b0fd4380f0cf915021fedf0400718f3a8d3440f85052c49eee2\n","  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=65db95c731484c158ebd03e88b521f48f86bf42bb1867b797894cc8b6d6b063e\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=21e2f80b033ca0505a33fb67fc44657373a66beb00f61ba1c481ce34fdd30df2\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built object-detection avro-python3 crcmod dill pyyaml seqeval docopt\n","Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, sentencepiece, keras, docopt, crcmod, zstandard, typeguard, tensorflow_io, tensorboard-data-server, pyyaml, portalocker, orjson, objsize, numpy, fasteners, fastavro, dnspython, dill, colorama, avro-python3, tensorflow-model-optimization, tensorflow-addons, sacrebleu, pymongo, keras-preprocessing, hdfs, google-auth-oauthlib, apache-beam, tensorboard, seqeval, lvis, tensorflow, tensorflow-text, tf-models-official, object-detection\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.1\n","    Uninstalling tensorboard-data-server-0.7.1:\n","      Successfully uninstalled tensorboard-data-server-0.7.1\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.3\n","    Uninstalling tensorboard-2.12.3:\n","      Successfully uninstalled tensorboard-2.12.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed apache-beam-2.48.0 avro-python3-1.10.2 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.3.0 docopt-0.6.2 fastavro-1.7.4 fasteners-0.18 google-auth-oauthlib-0.4.6 hdfs-2.7.0 keras-2.8.0 keras-preprocessing-1.1.2 lvis-0.5.3 numpy-1.24.4 object-detection-0.1 objsize-0.6.1 orjson-3.9.1 portalocker-2.7.0 pymongo-4.4.0 pyyaml-5.4.1 sacrebleu-2.2.0 sentencepiece-0.1.99 seqeval-1.2.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.1 tensorflow-addons-0.20.0 tensorflow-estimator-2.8.0 tensorflow-model-optimization-0.7.5 tensorflow-text-2.8.2 tensorflow_io-0.32.0 tf-models-official-2.8.0 typeguard-2.13.3 zstandard-0.21.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","yaml"]}}},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Collecting tensorflow==2.8.0\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.8.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.24.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.6.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.32.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.56.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.40.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.10)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorflow\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.1\n","    Uninstalling tensorflow-2.8.1:\n","      Successfully uninstalled tensorflow-2.8.1\n","Successfully installed tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"]}],"source":["# Install the Object Detection API\n","!pip install /content/models/research/\n","\n","# Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)\n","!pip install tensorflow==2.8.0"]},{"cell_type":"markdown","metadata":{"id":"6V7TrfUos-9E"},"source":["You may get warnings or errors related to package dependencies in the previous code block, but you can ignore them for now.\n","\n","Let's test our installation by running `model_builder_tf2_test.py` to make sure everything is working as expected. Run the following code block and confirm that it finishes without errors. If you get errors, try Googling them or checking the FAQ at the end of this Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wh_HPMOqWH9z"},"outputs":[],"source":["# Run Model Bulider Test file, just to verify everything's working properly\n","!python /content/models/research/object_detection/builders/model_builder_tf2_test.py\n"]},{"cell_type":"markdown","metadata":{"id":"ZoRdB64PEn6V"},"source":["# 3.&nbsp;Download Image Dataset and Prepare Training Data\n","\n","We download our custom dataset in Tensorflow TFRecord format"]},{"cell_type":"code","source":["!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"ekJ5gEYpmUgNswc0SuHe\")\n","project = rf.workspace(\"lamao\").project(\"sod-enect\")\n","version = project.version(2)\n","dataset = version.download(\"tfrecord\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GhdB4rjCACMn","executionInfo":{"status":"ok","timestamp":1701897667695,"user_tz":480,"elapsed":79816,"user":{"displayName":"YSMN SHOP","userId":"14957957093939577516"}},"outputId":"87380ff4-8f54-4f2a-cc12-925e6f5000d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting roboflow\n","  Downloading roboflow-1.1.12-py3-none-any.whl (68 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting certifi==2023.7.22 (from roboflow)\n","  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n","  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n","Collecting idna==2.10 (from roboflow)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n","Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n","  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n","Collecting pyparsing==2.4.7 (from roboflow)\n","  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Collecting python-dotenv (from roboflow)\n","  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Collecting supervision (from roboflow)\n","  Downloading supervision-0.17.0-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n","Collecting requests-toolbelt (from roboflow)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-magic (from roboflow)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.46.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n","Requirement already satisfied: scipy<=2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.4)\n","Installing collected packages: python-magic, python-dotenv, pyparsing, opencv-python-headless, idna, cycler, chardet, certifi, supervision, requests-toolbelt, roboflow\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.1.1\n","    Uninstalling pyparsing-3.1.1:\n","      Successfully uninstalled pyparsing-3.1.1\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.8.1.78\n","    Uninstalling opencv-python-headless-4.8.1.78:\n","      Successfully uninstalled opencv-python-headless-4.8.1.78\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.6\n","    Uninstalling idna-3.6:\n","      Successfully uninstalled idna-3.6\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.12.1\n","    Uninstalling cycler-0.12.1:\n","      Successfully uninstalled cycler-0.12.1\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2023.11.17\n","    Uninstalling certifi-2023.11.17:\n","      Successfully uninstalled certifi-2023.11.17\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed certifi-2023.7.22 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 pyparsing-2.4.7 python-dotenv-1.0.0 python-magic-0.4.27 requests-toolbelt-1.0.0 roboflow-1.1.12 supervision-0.17.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["certifi","cycler","pyparsing"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in OOD-2 to tfrecord:: 100%|██████████| 2369188/2369188 [00:55<00:00, 42435.49it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to OOD-2 in tfrecord:: 100%|██████████| 11/11 [00:09<00:00,  1.18it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"qQV1nzifEn6Y"},"source":["### 3.1 Create Labelmap and upload TFRecords files\n","We need to define  a labelmap of our classes for the detector and upload TFRecords files, which are used by TensorFlow for training.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nDnaOjwEn6Z"},"outputs":[],"source":["### This creates a a \"labelmap.txt\" file with a list of classes the object detection model will detect.\n","%%bash\n","cat <<EOF >> /content/SOD-2/labelmap.txt\n","bench\n","bicycle\n","bus\n","bus_stop\n","cane\n","car\n","curb\n","dog\n","fire_hydrant\n","motorcycle\n","person\n","pole\n","spherical_roadblock\n","stairs\n","stop_sign\n","street_light\n","traffic_light\n","train\n","tree\n","truck\n","warning_column\n","waste_container\n","EOF"]},{"cell_type":"markdown","metadata":{"id":"doKHsBvuEn6b"},"source":["We'll store the locations of the TFRecord and labelmap files as variables so we can reference them later in this Colab session."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJgIK8OrEn6b"},"outputs":[],"source":["train_record_fname = '/content/SOD-2/train/train.tfrecord'\n","val_record_fname = '/content//SOD-2/valid/val.tfrecord'\n","label_map_pbtxt_fname = '/content/SOD-2/labelmap.pbtxt'"]},{"cell_type":"markdown","metadata":{"id":"eGEUZYAMEZ6f"},"source":["# 4.&nbsp;Set Up Training Configuration"]},{"cell_type":"markdown","metadata":{"id":"I2MAcgJ53STW"},"source":["In this section, we'll set up the model and training configuration. We'll specifiy which pretrained TensorFlow model we want to use from the [TensorFlow 2 Object Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Each model also comes with a configuration file that points to file locations, sets training parameters (such as learning rate and total number of training steps), and more. We'll modify the configuration file for our custom training job.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN0EUEa3e5Un"},"outputs":[],"source":["# Change the chosen_model variable to deploy different models available in the TF2 object detection zoo\n","chosen_model = 'efficientdet-d1'\n","\n","MODELS_CONFIG = {\n","    'ssd-mobilenet-v2': {\n","        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n","        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n","    },\n","    'efficientdet-d0': {\n","        'model_name': 'efficientdet_d0_coco17_tpu-32',\n","        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n","    },\n","    'ssd-mobilenet-v2-fpnlite-320': {\n","        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n","        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n","    },\n","     'efficientdet-d1': {\n","        'model_name': 'efficientdet_d1_coco17_tpu-32',\n","        'base_pipeline_file': 'ssd_efficientdet_d1_640x640_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'efficientdet_d1_coco17_tpu-32.tar.gz',\n","     }\n","}\n","\n","model_name = MODELS_CONFIG[chosen_model]['model_name']\n","pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n","base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"]},{"cell_type":"markdown","metadata":{"id":"JMG3EEPqPggV"},"source":["Download the pretrained model file and configuration file by clicking Play on the following section."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1801,"status":"ok","timestamp":1686139470412,"user":{"displayName":"Ahmed Boussihmed","userId":"16936910001719053027"},"user_tz":-60},"id":"kG4TmJUVrYQ7","outputId":"fd6b0c82-2f21-4a42-e6d5-7799d1b0abb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","/content/models/mymodel\n","--2023-06-07 12:04:28--  http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d1_coco17_tpu-32.tar.gz\n","Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.126.128, 2a00:1450:4013:c07::80\n","Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.126.128|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 51839363 (49M) [application/x-tar]\n","Saving to: ‘efficientdet_d1_coco17_tpu-32.tar.gz’\n","\n","efficientdet_d1_coc 100%[===================>]  49.44M   238MB/s    in 0.2s    \n","\n","2023-06-07 12:04:29 (238 MB/s) - ‘efficientdet_d1_coco17_tpu-32.tar.gz’ saved [51839363/51839363]\n","\n","--2023-06-07 12:04:29--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_efficientdet_d1_640x640_coco17_tpu-8.config\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4630 (4.5K) [text/plain]\n","Saving to: ‘ssd_efficientdet_d1_640x640_coco17_tpu-8.config’\n","\n","ssd_efficientdet_d1 100%[===================>]   4.52K  --.-KB/s    in 0s      \n","\n","2023-06-07 12:04:30 (53.7 MB/s) - ‘ssd_efficientdet_d1_640x640_coco17_tpu-8.config’ saved [4630/4630]\n","\n"]}],"source":["# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n","%mkdir /content/models/mymodel/\n","%cd /content/models/mymodel/\n","\n","# Download pre-trained model weights\n","import tarfile\n","download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n","!wget {download_tar}\n","tar = tarfile.open(pretrained_checkpoint)\n","tar.extractall()\n","tar.close()\n","\n","# Download training configuration file for model\n","download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n","!wget {download_config}"]},{"cell_type":"markdown","metadata":{"id":"BFAlqNrPn5y3"},"source":["Now that we've downloaded our model and config file, we need to modify the configuration file with some high-level training parameters. The following variables are used to control training steps:\n","\n","* **num_steps**: The total amount of steps to use for training the model. A good number to start with is 40,000 steps. You can use more steps if you notice the loss metrics are still decreasing by the time training finishes. The more steps, the longer training will take. Training can also be stopped early if loss flattens out before reaching the specified number of steps.\n","* **batch_size**: The number of images to use per training step. A larger batch size allows a model to be trained in fewer steps, but the size is limited by the GPU memory available for training. With the GPUs used in Colab instances, 16 is a good number for SSD models and 4 is good for EfficientDet models.\n","\n","Other training information, like the location of the pretrained model file, the config file, and total number of classes are also assigned in this step. To learn more about training configuration with the TensorFlow Object Detection API, read this [article from Neptune](https://neptune.ai/blog/tensorflow-object-detection-api-best-practices-to-training-evaluation-deployment)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1lYDvJN-n69v"},"outputs":[],"source":["# Set training parameters for the model\n","num_steps = 50000\n","\n","if chosen_model == 'efficientdet-d1':\n","  batch_size = 4\n","else:\n","  batch_size = 16"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":345,"status":"ok","timestamp":1686140267927,"user":{"displayName":"Ahmed Boussihmed","userId":"16936910001719053027"},"user_tz":-60},"id":"b_ki9jOqxn7V","outputId":"06a35e72-fd20-4db7-8d81-d6eb1f772ed4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total classes: 22\n"]}],"source":["# Set file locations and get number of classes for config file\n","pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n","fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n","\n","def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    return len(category_index.keys())\n","num_classes = get_num_classes(label_map_pbtxt_fname)\n","print('Total classes:', num_classes)\n"]},{"cell_type":"markdown","metadata":{"id":"cwPyaIAXxyKu"},"source":["Next, we'll rewrite the config file to use the training parameters we just specified. The following section of code will automatically replace the necessary parameters in the downloaded .config file and save it as our custom \"pipeline_file.config\" file."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":461,"status":"ok","timestamp":1686140284858,"user":{"displayName":"Ahmed Boussihmed","userId":"16936910001719053027"},"user_tz":-60},"id":"5eA5ht3_yukT","outputId":"920631ac-b4e4-4a82-8032-c7e40fb37b1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/models/mymodel\n","writing custom configuration file\n"]}],"source":["# Create custom configuration file by writing the dataset, model checkpoint, and training parameters into the base pipeline file\n","import re\n","\n","%cd /content/models/mymodel\n","print('writing custom configuration file')\n","\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","with open('pipeline_file.config', 'w') as f:\n","\n","    # Set fine_tune_checkpoint path\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","\n","    # Set tfrecord files for train and test datasets\n","    s = re.sub(\n","        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n","\n","    # Set label_map_path\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # Set batch_size\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    # Set training steps, num_steps\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","\n","    # Set number of classes num_classes\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","\n","    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n","    s = re.sub(\n","        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n","\n","    # If using ssd-mobilenet-v2, reduce learning rate (because it's too high in the default config file)\n","    if chosen_model == 'ssd-mobilenet-v2':\n","      s = re.sub('learning_rate_base: .8',\n","                 'learning_rate_base: .08', s)\n","\n","      s = re.sub('warmup_learning_rate: 0.13333',\n","                 'warmup_learning_rate: .026666', s)\n","\n","    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n","    if chosen_model == 'efficientdet-d0':\n","      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n","      s = re.sub('pad_to_max_dimension: true', '', s)\n","      s = re.sub('min_dimension', 'height', s)\n","      s = re.sub('max_dimension', 'width', s)\n","\n","    f.write(s)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEsOLOMHzBqF"},"outputs":[],"source":["# Display the custom configuration file's contents\n","!cat /content/models/mymodel/pipeline_file.config"]},{"cell_type":"markdown","metadata":{"id":"UXpnXYC908Zl"},"source":["Finally, let's set the locations of the configuration file and model output directory as variables so we can reference them when we call the training command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMlaN3rs3zLe"},"outputs":[],"source":["# Set the path to the custom config file and the directory to store training checkpoints in\n","pipeline_file = '/content/models/mymodel/pipeline_file.config'\n","model_dir = '/content/training/train'"]},{"cell_type":"markdown","metadata":{"id":"-19zML6oEO7l"},"source":["# 5.&nbsp;Train Custom TFLite Detection Model"]},{"cell_type":"markdown","metadata":{"id":"XxPj_QV43qD5"},"source":["We're ready to train our object detection model! Before we start training, let's load up a TensorBoard session to monitor training progress. Run the following section of code, and a TensorBoard session will appear in the browser. It won't show anything yet, because we haven't started training. Once training starts, come back and click the refresh button to see the model's overall loss.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TI9iCCxoNlAL"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir '/content/training/train'"]},{"cell_type":"markdown","metadata":{"id":"5cuQpPJL2pUq"},"source":["Model training is performed using the \"model_main_tf2.py\" script from the TF Object Detection API. Training will take anywhere from 2 to 6 hours, depending on the model, batch size, and number of training steps. We've already defined all the parameters and arguments used by `model_main_tf2.py` in previous sections of this Colab. Just click Play on the following block to begin training!\n","\n","\n","\n","> *Note: It takes a few minutes for the program to display any training messages, because it only displays logs once every 100 steps. If it seems like nothing is happening, just wait a couple minutes.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQTfZChVzzpZ","outputId":"0ca28653-c46a-400c-f07c-c449336addfa"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl5mutexC1Ev']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFN3tsl8StatusOrISt10unique_ptrIS1_NS5_4core15RefCountDeleterEEEEvEE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  warnings.warn(\n","2023-06-07 12:18:29.590198: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","I0607 12:18:29.647677 139945423042368 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","INFO:tensorflow:Maybe overwriting train_steps: 50000\n","I0607 12:18:29.653551 139945423042368 config_util.py:552] Maybe overwriting train_steps: 50000\n","INFO:tensorflow:Maybe overwriting use_bfloat16: False\n","I0607 12:18:29.653751 139945423042368 config_util.py:552] Maybe overwriting use_bfloat16: False\n","I0607 12:18:29.671751 139945423042368 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\n","I0607 12:18:29.671969 139945423042368 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\n","I0607 12:18:29.672090 139945423042368 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\n","I0607 12:18:29.678275 139945423042368 efficientnet_model.py:144] round_filter input=32 output=32\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.737305 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.754984 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.758379 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.759771 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.771448 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.776355 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.785148 139945423042368 efficientnet_model.py:144] round_filter input=32 output=32\n","I0607 12:18:29.785296 139945423042368 efficientnet_model.py:144] round_filter input=16 output=16\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.811527 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.813118 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.815695 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:29.817090 139945423042368 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0607 12:18:30.103993 139945423042368 efficientnet_model.py:144] round_filter input=16 output=16\n","I0607 12:18:30.104217 139945423042368 efficientnet_model.py:144] round_filter input=24 output=24\n","I0607 12:18:30.690166 139945423042368 efficientnet_model.py:144] round_filter input=24 output=24\n","I0607 12:18:30.690386 139945423042368 efficientnet_model.py:144] round_filter input=40 output=40\n","I0607 12:18:31.279885 139945423042368 efficientnet_model.py:144] round_filter input=40 output=40\n","I0607 12:18:31.280125 139945423042368 efficientnet_model.py:144] round_filter input=80 output=80\n","I0607 12:18:32.107987 139945423042368 efficientnet_model.py:144] round_filter input=80 output=80\n","I0607 12:18:32.108170 139945423042368 efficientnet_model.py:144] round_filter input=112 output=112\n","I0607 12:18:32.638134 139945423042368 efficientnet_model.py:144] round_filter input=112 output=112\n","I0607 12:18:32.638307 139945423042368 efficientnet_model.py:144] round_filter input=192 output=192\n","I0607 12:18:33.306204 139945423042368 efficientnet_model.py:144] round_filter input=192 output=192\n","I0607 12:18:33.306395 139945423042368 efficientnet_model.py:144] round_filter input=320 output=320\n","I0607 12:18:33.561101 139945423042368 efficientnet_model.py:144] round_filter input=1280 output=1280\n","I0607 12:18:33.626520 139945423042368 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","rename to distribute_datasets_from_function\n","W0607 12:18:33.675967 139945423042368 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","rename to distribute_datasets_from_function\n","INFO:tensorflow:Reading unweighted datasets: ['/content/drive/MyDrive/OOD/train/train.tfrecord']\n","I0607 12:18:33.702458 139945423042368 dataset_builder.py:162] Reading unweighted datasets: ['/content/drive/MyDrive/OOD/train/train.tfrecord']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/drive/MyDrive/OOD/train/train.tfrecord']\n","I0607 12:18:33.703646 139945423042368 dataset_builder.py:79] Reading record datasets for input file: ['/content/drive/MyDrive/OOD/train/train.tfrecord']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0607 12:18:33.703811 139945423042368 dataset_builder.py:80] Number of filenames to read: 1\n","WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n","W0607 12:18:33.703903 139945423042368 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n","W0607 12:18:33.709787 139945423042368 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","W0607 12:18:33.742805 139945423042368 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","W0607 12:18:40.301550 139945423042368 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0607 12:18:45.721722 139945423042368 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","/usr/local/lib/python3.10/dist-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n","  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n","WARNING:tensorflow:AutoGraph could not transform <bound method PercentStyle._format of <logging.PercentStyle object at 0x7f47945e1ab0>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: 'NoneType' object has no attribute '_fields'\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","W0607 12:19:02.753091 139939480033024 ag_logging.py:142] AutoGraph could not transform <bound method PercentStyle._format of <logging.PercentStyle object at 0x7f47945e1ab0>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: 'NoneType' object has no attribute '_fields'\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","I0607 12:19:01.548934 139939480033024 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","I0607 12:19:18.818008 139939480033024 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","W0607 12:19:46.409397 139939496818432 deprecation.py:541] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","I0607 12:19:49.351609 139939496818432 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","W0607 12:20:00.662502 139939496818432 utils.py:76] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","I0607 12:20:07.850360 139939496818432 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","W0607 12:20:16.452642 139939496818432 utils.py:76] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","I0607 12:20:24.333354 139939496818432 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","W0607 12:20:31.711507 139939496818432 utils.py:76] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","I0607 12:20:40.036978 139939496818432 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","W0607 12:20:49.894460 139939496818432 utils.py:76] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","INFO:tensorflow:Step 100 per-step time 1.730s\n","I0607 12:22:39.052304 139945423042368 model_lib_v2.py:705] Step 100 per-step time 1.730s\n","INFO:tensorflow:{'Loss/classification_loss': 1.6450961,\n"," 'Loss/localization_loss': 0.59445786,\n"," 'Loss/regularization_loss': 0.029537028,\n"," 'Loss/total_loss': 2.269091,\n"," 'learning_rate': 0.00416}\n","I0607 12:22:39.052819 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.6450961,\n"," 'Loss/localization_loss': 0.59445786,\n"," 'Loss/regularization_loss': 0.029537028,\n"," 'Loss/total_loss': 2.269091,\n"," 'learning_rate': 0.00416}\n","INFO:tensorflow:Step 200 per-step time 0.643s\n","I0607 12:23:43.383280 139945423042368 model_lib_v2.py:705] Step 200 per-step time 0.643s\n","INFO:tensorflow:{'Loss/classification_loss': 1.4247673,\n"," 'Loss/localization_loss': 0.46160942,\n"," 'Loss/regularization_loss': 0.029558517,\n"," 'Loss/total_loss': 1.9159352,\n"," 'learning_rate': 0.0073200003}\n","I0607 12:23:43.383738 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.4247673,\n"," 'Loss/localization_loss': 0.46160942,\n"," 'Loss/regularization_loss': 0.029558517,\n"," 'Loss/total_loss': 1.9159352,\n"," 'learning_rate': 0.0073200003}\n","INFO:tensorflow:Step 300 per-step time 0.650s\n","I0607 12:24:48.407479 139945423042368 model_lib_v2.py:705] Step 300 per-step time 0.650s\n","INFO:tensorflow:{'Loss/classification_loss': 1.6649292,\n"," 'Loss/localization_loss': 0.6154495,\n"," 'Loss/regularization_loss': 0.0296268,\n"," 'Loss/total_loss': 2.3100054,\n"," 'learning_rate': 0.010480001}\n","I0607 12:24:48.407945 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.6649292,\n"," 'Loss/localization_loss': 0.6154495,\n"," 'Loss/regularization_loss': 0.0296268,\n"," 'Loss/total_loss': 2.3100054,\n"," 'learning_rate': 0.010480001}\n","INFO:tensorflow:Step 400 per-step time 0.649s\n","I0607 12:25:53.254496 139945423042368 model_lib_v2.py:705] Step 400 per-step time 0.649s\n","INFO:tensorflow:{'Loss/classification_loss': 1.3466501,\n"," 'Loss/localization_loss': 0.6198501,\n"," 'Loss/regularization_loss': 0.029818323,\n"," 'Loss/total_loss': 1.9963186,\n"," 'learning_rate': 0.0136400005}\n","I0607 12:25:53.254900 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.3466501,\n"," 'Loss/localization_loss': 0.6198501,\n"," 'Loss/regularization_loss': 0.029818323,\n"," 'Loss/total_loss': 1.9963186,\n"," 'learning_rate': 0.0136400005}\n","INFO:tensorflow:Step 500 per-step time 0.656s\n","I0607 12:26:58.907957 139945423042368 model_lib_v2.py:705] Step 500 per-step time 0.656s\n","INFO:tensorflow:{'Loss/classification_loss': 1.0764664,\n"," 'Loss/localization_loss': 0.34589288,\n"," 'Loss/regularization_loss': 0.030032197,\n"," 'Loss/total_loss': 1.4523915,\n"," 'learning_rate': 0.016800001}\n","I0607 12:26:58.908445 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.0764664,\n"," 'Loss/localization_loss': 0.34589288,\n"," 'Loss/regularization_loss': 0.030032197,\n"," 'Loss/total_loss': 1.4523915,\n"," 'learning_rate': 0.016800001}\n","INFO:tensorflow:Step 600 per-step time 0.662s\n","I0607 12:28:05.053498 139945423042368 model_lib_v2.py:705] Step 600 per-step time 0.662s\n","INFO:tensorflow:{'Loss/classification_loss': 1.0108774,\n"," 'Loss/localization_loss': 0.46760124,\n"," 'Loss/regularization_loss': 0.030295307,\n"," 'Loss/total_loss': 1.5087739,\n"," 'learning_rate': 0.019960001}\n","I0607 12:28:05.053925 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.0108774,\n"," 'Loss/localization_loss': 0.46760124,\n"," 'Loss/regularization_loss': 0.030295307,\n"," 'Loss/total_loss': 1.5087739,\n"," 'learning_rate': 0.019960001}\n","INFO:tensorflow:Step 700 per-step time 0.650s\n","I0607 12:29:10.148427 139945423042368 model_lib_v2.py:705] Step 700 per-step time 0.650s\n","INFO:tensorflow:{'Loss/classification_loss': 0.98443025,\n"," 'Loss/localization_loss': 0.52199864,\n"," 'Loss/regularization_loss': 0.030595489,\n"," 'Loss/total_loss': 1.5370245,\n"," 'learning_rate': 0.023120001}\n","I0607 12:29:10.148980 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.98443025,\n"," 'Loss/localization_loss': 0.52199864,\n"," 'Loss/regularization_loss': 0.030595489,\n"," 'Loss/total_loss': 1.5370245,\n"," 'learning_rate': 0.023120001}\n","INFO:tensorflow:Step 800 per-step time 0.657s\n","I0607 12:30:15.819353 139945423042368 model_lib_v2.py:705] Step 800 per-step time 0.657s\n","INFO:tensorflow:{'Loss/classification_loss': 1.1539701,\n"," 'Loss/localization_loss': 0.58526886,\n"," 'Loss/regularization_loss': 0.031077934,\n"," 'Loss/total_loss': 1.770317,\n"," 'learning_rate': 0.02628}\n","I0607 12:30:15.819697 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.1539701,\n"," 'Loss/localization_loss': 0.58526886,\n"," 'Loss/regularization_loss': 0.031077934,\n"," 'Loss/total_loss': 1.770317,\n"," 'learning_rate': 0.02628}\n","INFO:tensorflow:Step 900 per-step time 0.669s\n","I0607 12:31:22.730725 139945423042368 model_lib_v2.py:705] Step 900 per-step time 0.669s\n","INFO:tensorflow:{'Loss/classification_loss': 1.0108896,\n"," 'Loss/localization_loss': 0.42330134,\n"," 'Loss/regularization_loss': 0.031526983,\n"," 'Loss/total_loss': 1.465718,\n"," 'learning_rate': 0.02944}\n","I0607 12:31:22.731160 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.0108896,\n"," 'Loss/localization_loss': 0.42330134,\n"," 'Loss/regularization_loss': 0.031526983,\n"," 'Loss/total_loss': 1.465718,\n"," 'learning_rate': 0.02944}\n","INFO:tensorflow:Step 1000 per-step time 0.666s\n","I0607 12:32:29.325798 139945423042368 model_lib_v2.py:705] Step 1000 per-step time 0.666s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8181677,\n"," 'Loss/localization_loss': 0.32233986,\n"," 'Loss/regularization_loss': 0.031906467,\n"," 'Loss/total_loss': 1.1724141,\n"," 'learning_rate': 0.0326}\n","I0607 12:32:29.326157 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8181677,\n"," 'Loss/localization_loss': 0.32233986,\n"," 'Loss/regularization_loss': 0.031906467,\n"," 'Loss/total_loss': 1.1724141,\n"," 'learning_rate': 0.0326}\n","INFO:tensorflow:Step 1100 per-step time 0.679s\n","I0607 12:33:37.227591 139945423042368 model_lib_v2.py:705] Step 1100 per-step time 0.679s\n","INFO:tensorflow:{'Loss/classification_loss': 1.111568,\n"," 'Loss/localization_loss': 0.7610013,\n"," 'Loss/regularization_loss': 0.032347966,\n"," 'Loss/total_loss': 1.9049172,\n"," 'learning_rate': 0.03576}\n","I0607 12:33:37.228031 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.111568,\n"," 'Loss/localization_loss': 0.7610013,\n"," 'Loss/regularization_loss': 0.032347966,\n"," 'Loss/total_loss': 1.9049172,\n"," 'learning_rate': 0.03576}\n","INFO:tensorflow:Step 1200 per-step time 0.652s\n","I0607 12:34:42.474150 139945423042368 model_lib_v2.py:705] Step 1200 per-step time 0.652s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8049897,\n"," 'Loss/localization_loss': 0.38051617,\n"," 'Loss/regularization_loss': 0.032896657,\n"," 'Loss/total_loss': 1.2184025,\n"," 'learning_rate': 0.03892}\n","I0607 12:34:42.474602 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8049897,\n"," 'Loss/localization_loss': 0.38051617,\n"," 'Loss/regularization_loss': 0.032896657,\n"," 'Loss/total_loss': 1.2184025,\n"," 'learning_rate': 0.03892}\n","INFO:tensorflow:Step 1300 per-step time 0.651s\n","I0607 12:35:47.521684 139945423042368 model_lib_v2.py:705] Step 1300 per-step time 0.651s\n","INFO:tensorflow:{'Loss/classification_loss': 0.848128,\n"," 'Loss/localization_loss': 0.3510054,\n"," 'Loss/regularization_loss': 0.03331296,\n"," 'Loss/total_loss': 1.2324463,\n"," 'learning_rate': 0.04208}\n","I0607 12:35:47.522116 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.848128,\n"," 'Loss/localization_loss': 0.3510054,\n"," 'Loss/regularization_loss': 0.03331296,\n"," 'Loss/total_loss': 1.2324463,\n"," 'learning_rate': 0.04208}\n","INFO:tensorflow:Step 1400 per-step time 0.661s\n","I0607 12:36:53.632444 139945423042368 model_lib_v2.py:705] Step 1400 per-step time 0.661s\n","INFO:tensorflow:{'Loss/classification_loss': 0.7394772,\n"," 'Loss/localization_loss': 0.35598323,\n"," 'Loss/regularization_loss': 0.03376853,\n"," 'Loss/total_loss': 1.129229,\n"," 'learning_rate': 0.04524}\n","I0607 12:36:53.632915 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.7394772,\n"," 'Loss/localization_loss': 0.35598323,\n"," 'Loss/regularization_loss': 0.03376853,\n"," 'Loss/total_loss': 1.129229,\n"," 'learning_rate': 0.04524}\n","INFO:tensorflow:Step 1500 per-step time 0.657s\n","I0607 12:37:59.298168 139945423042368 model_lib_v2.py:705] Step 1500 per-step time 0.657s\n","INFO:tensorflow:{'Loss/classification_loss': 0.82089794,\n"," 'Loss/localization_loss': 0.3231027,\n"," 'Loss/regularization_loss': 0.034218326,\n"," 'Loss/total_loss': 1.178219,\n"," 'learning_rate': 0.0484}\n","I0607 12:37:59.298491 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.82089794,\n"," 'Loss/localization_loss': 0.3231027,\n"," 'Loss/regularization_loss': 0.034218326,\n"," 'Loss/total_loss': 1.178219,\n"," 'learning_rate': 0.0484}\n","INFO:tensorflow:Step 1600 per-step time 0.652s\n","I0607 12:39:04.536166 139945423042368 model_lib_v2.py:705] Step 1600 per-step time 0.652s\n","INFO:tensorflow:{'Loss/classification_loss': 0.97549725,\n"," 'Loss/localization_loss': 0.45113674,\n"," 'Loss/regularization_loss': 0.03466465,\n"," 'Loss/total_loss': 1.4612986,\n"," 'learning_rate': 0.05156}\n","I0607 12:39:04.536551 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.97549725,\n"," 'Loss/localization_loss': 0.45113674,\n"," 'Loss/regularization_loss': 0.03466465,\n"," 'Loss/total_loss': 1.4612986,\n"," 'learning_rate': 0.05156}\n","INFO:tensorflow:Step 1700 per-step time 0.650s\n","I0607 12:40:09.525269 139945423042368 model_lib_v2.py:705] Step 1700 per-step time 0.650s\n","INFO:tensorflow:{'Loss/classification_loss': 0.9152931,\n"," 'Loss/localization_loss': 0.2857435,\n"," 'Loss/regularization_loss': 0.03515027,\n"," 'Loss/total_loss': 1.2361869,\n"," 'learning_rate': 0.05472}\n","I0607 12:40:09.525586 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.9152931,\n"," 'Loss/localization_loss': 0.2857435,\n"," 'Loss/regularization_loss': 0.03515027,\n"," 'Loss/total_loss': 1.2361869,\n"," 'learning_rate': 0.05472}\n","INFO:tensorflow:Step 1800 per-step time 0.654s\n","I0607 12:41:14.930719 139945423042368 model_lib_v2.py:705] Step 1800 per-step time 0.654s\n","INFO:tensorflow:{'Loss/classification_loss': 1.2924455,\n"," 'Loss/localization_loss': 0.77269804,\n"," 'Loss/regularization_loss': 0.035728704,\n"," 'Loss/total_loss': 2.1008723,\n"," 'learning_rate': 0.05788}\n","I0607 12:41:14.931232 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.2924455,\n"," 'Loss/localization_loss': 0.77269804,\n"," 'Loss/regularization_loss': 0.035728704,\n"," 'Loss/total_loss': 2.1008723,\n"," 'learning_rate': 0.05788}\n","INFO:tensorflow:Step 1900 per-step time 0.647s\n","I0607 12:42:19.585321 139945423042368 model_lib_v2.py:705] Step 1900 per-step time 0.647s\n","INFO:tensorflow:{'Loss/classification_loss': 0.94320726,\n"," 'Loss/localization_loss': 0.37703183,\n"," 'Loss/regularization_loss': 0.03620754,\n"," 'Loss/total_loss': 1.3564466,\n"," 'learning_rate': 0.06104}\n","I0607 12:42:19.585698 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.94320726,\n"," 'Loss/localization_loss': 0.37703183,\n"," 'Loss/regularization_loss': 0.03620754,\n"," 'Loss/total_loss': 1.3564466,\n"," 'learning_rate': 0.06104}\n","INFO:tensorflow:Step 2000 per-step time 0.649s\n","I0607 12:43:24.458756 139945423042368 model_lib_v2.py:705] Step 2000 per-step time 0.649s\n","INFO:tensorflow:{'Loss/classification_loss': 0.71829283,\n"," 'Loss/localization_loss': 0.31926146,\n"," 'Loss/regularization_loss': 0.036761098,\n"," 'Loss/total_loss': 1.0743153,\n"," 'learning_rate': 0.06420001}\n","I0607 12:43:24.459204 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.71829283,\n"," 'Loss/localization_loss': 0.31926146,\n"," 'Loss/regularization_loss': 0.036761098,\n"," 'Loss/total_loss': 1.0743153,\n"," 'learning_rate': 0.06420001}\n","INFO:tensorflow:Step 2100 per-step time 0.669s\n","I0607 12:44:31.386052 139945423042368 model_lib_v2.py:705] Step 2100 per-step time 0.669s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8030942,\n"," 'Loss/localization_loss': 0.31869802,\n"," 'Loss/regularization_loss': 0.03733329,\n"," 'Loss/total_loss': 1.1591256,\n"," 'learning_rate': 0.067360006}\n","I0607 12:44:31.386407 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8030942,\n"," 'Loss/localization_loss': 0.31869802,\n"," 'Loss/regularization_loss': 0.03733329,\n"," 'Loss/total_loss': 1.1591256,\n"," 'learning_rate': 0.067360006}\n","INFO:tensorflow:Step 2200 per-step time 0.645s\n","I0607 12:45:35.846339 139945423042368 model_lib_v2.py:705] Step 2200 per-step time 0.645s\n","INFO:tensorflow:{'Loss/classification_loss': 0.81681716,\n"," 'Loss/localization_loss': 0.35786164,\n"," 'Loss/regularization_loss': 0.037888438,\n"," 'Loss/total_loss': 1.2125672,\n"," 'learning_rate': 0.070520006}\n","I0607 12:45:35.846819 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.81681716,\n"," 'Loss/localization_loss': 0.35786164,\n"," 'Loss/regularization_loss': 0.037888438,\n"," 'Loss/total_loss': 1.2125672,\n"," 'learning_rate': 0.070520006}\n","INFO:tensorflow:Step 2300 per-step time 0.652s\n","I0607 12:46:41.058249 139945423042368 model_lib_v2.py:705] Step 2300 per-step time 0.652s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8716505,\n"," 'Loss/localization_loss': 0.55902666,\n"," 'Loss/regularization_loss': 0.038627326,\n"," 'Loss/total_loss': 1.4693046,\n"," 'learning_rate': 0.073680006}\n","I0607 12:46:41.058694 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8716505,\n"," 'Loss/localization_loss': 0.55902666,\n"," 'Loss/regularization_loss': 0.038627326,\n"," 'Loss/total_loss': 1.4693046,\n"," 'learning_rate': 0.073680006}\n","INFO:tensorflow:Step 2400 per-step time 0.648s\n","I0607 12:47:45.816279 139945423042368 model_lib_v2.py:705] Step 2400 per-step time 0.648s\n","INFO:tensorflow:{'Loss/classification_loss': 0.88709337,\n"," 'Loss/localization_loss': 0.37852547,\n"," 'Loss/regularization_loss': 0.0392064,\n"," 'Loss/total_loss': 1.3048252,\n"," 'learning_rate': 0.076840006}\n","I0607 12:47:45.816700 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.88709337,\n"," 'Loss/localization_loss': 0.37852547,\n"," 'Loss/regularization_loss': 0.0392064,\n"," 'Loss/total_loss': 1.3048252,\n"," 'learning_rate': 0.076840006}\n","INFO:tensorflow:Step 2500 per-step time 0.647s\n","I0607 12:48:50.517371 139945423042368 model_lib_v2.py:705] Step 2500 per-step time 0.647s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8425241,\n"," 'Loss/localization_loss': 0.2107898,\n"," 'Loss/regularization_loss': 0.03978407,\n"," 'Loss/total_loss': 1.093098,\n"," 'learning_rate': 0.08}\n","I0607 12:48:50.517799 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8425241,\n"," 'Loss/localization_loss': 0.2107898,\n"," 'Loss/regularization_loss': 0.03978407,\n"," 'Loss/total_loss': 1.093098,\n"," 'learning_rate': 0.08}\n","INFO:tensorflow:Step 2600 per-step time 0.648s\n","I0607 12:49:55.330477 139945423042368 model_lib_v2.py:705] Step 2600 per-step time 0.648s\n","INFO:tensorflow:{'Loss/classification_loss': 1.0376844,\n"," 'Loss/localization_loss': 0.46591815,\n"," 'Loss/regularization_loss': 0.040440038,\n"," 'Loss/total_loss': 1.5440427,\n"," 'learning_rate': 0.079999976}\n","I0607 12:49:55.330816 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.0376844,\n"," 'Loss/localization_loss': 0.46591815,\n"," 'Loss/regularization_loss': 0.040440038,\n"," 'Loss/total_loss': 1.5440427,\n"," 'learning_rate': 0.079999976}\n","INFO:tensorflow:Step 2700 per-step time 0.652s\n","I0607 12:51:00.543298 139945423042368 model_lib_v2.py:705] Step 2700 per-step time 0.652s\n","INFO:tensorflow:{'Loss/classification_loss': 1.0251341,\n"," 'Loss/localization_loss': 0.5302785,\n"," 'Loss/regularization_loss': 0.04113982,\n"," 'Loss/total_loss': 1.5965524,\n"," 'learning_rate': 0.07999991}\n","I0607 12:51:00.543661 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.0251341,\n"," 'Loss/localization_loss': 0.5302785,\n"," 'Loss/regularization_loss': 0.04113982,\n"," 'Loss/total_loss': 1.5965524,\n"," 'learning_rate': 0.07999991}\n","INFO:tensorflow:Step 2800 per-step time 0.644s\n","I0607 12:52:04.907613 139945423042368 model_lib_v2.py:705] Step 2800 per-step time 0.644s\n","INFO:tensorflow:{'Loss/classification_loss': 0.9325378,\n"," 'Loss/localization_loss': 0.45228857,\n"," 'Loss/regularization_loss': 0.04176761,\n"," 'Loss/total_loss': 1.426594,\n"," 'learning_rate': 0.0799998}\n","I0607 12:52:04.907936 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.9325378,\n"," 'Loss/localization_loss': 0.45228857,\n"," 'Loss/regularization_loss': 0.04176761,\n"," 'Loss/total_loss': 1.426594,\n"," 'learning_rate': 0.0799998}\n","INFO:tensorflow:Step 2900 per-step time 0.641s\n","I0607 12:53:09.046205 139945423042368 model_lib_v2.py:705] Step 2900 per-step time 0.641s\n","INFO:tensorflow:{'Loss/classification_loss': 0.94825286,\n"," 'Loss/localization_loss': 0.3505477,\n"," 'Loss/regularization_loss': 0.042315017,\n"," 'Loss/total_loss': 1.3411156,\n"," 'learning_rate': 0.07999964}\n","I0607 12:53:09.046659 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.94825286,\n"," 'Loss/localization_loss': 0.3505477,\n"," 'Loss/regularization_loss': 0.042315017,\n"," 'Loss/total_loss': 1.3411156,\n"," 'learning_rate': 0.07999964}\n","INFO:tensorflow:Step 3000 per-step time 0.648s\n","I0607 12:54:13.857998 139945423042368 model_lib_v2.py:705] Step 3000 per-step time 0.648s\n","INFO:tensorflow:{'Loss/classification_loss': 0.82546073,\n"," 'Loss/localization_loss': 0.61874056,\n"," 'Loss/regularization_loss': 0.042906918,\n"," 'Loss/total_loss': 1.4871082,\n"," 'learning_rate': 0.07999944}\n","I0607 12:54:13.858399 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.82546073,\n"," 'Loss/localization_loss': 0.61874056,\n"," 'Loss/regularization_loss': 0.042906918,\n"," 'Loss/total_loss': 1.4871082,\n"," 'learning_rate': 0.07999944}\n","INFO:tensorflow:Step 3100 per-step time 0.678s\n","I0607 12:55:21.663241 139945423042368 model_lib_v2.py:705] Step 3100 per-step time 0.678s\n","INFO:tensorflow:{'Loss/classification_loss': 0.98593414,\n"," 'Loss/localization_loss': 0.44665614,\n"," 'Loss/regularization_loss': 0.043403763,\n"," 'Loss/total_loss': 1.475994,\n"," 'learning_rate': 0.07999919}\n","I0607 12:55:21.663720 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.98593414,\n"," 'Loss/localization_loss': 0.44665614,\n"," 'Loss/regularization_loss': 0.043403763,\n"," 'Loss/total_loss': 1.475994,\n"," 'learning_rate': 0.07999919}\n","INFO:tensorflow:Step 3200 per-step time 0.644s\n","I0607 12:56:26.077747 139945423042368 model_lib_v2.py:705] Step 3200 per-step time 0.644s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6219944,\n"," 'Loss/localization_loss': 0.3846004,\n"," 'Loss/regularization_loss': 0.043968793,\n"," 'Loss/total_loss': 1.0505636,\n"," 'learning_rate': 0.0799989}\n","I0607 12:56:26.078085 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.6219944,\n"," 'Loss/localization_loss': 0.3846004,\n"," 'Loss/regularization_loss': 0.043968793,\n"," 'Loss/total_loss': 1.0505636,\n"," 'learning_rate': 0.0799989}\n","INFO:tensorflow:Step 3300 per-step time 0.645s\n","I0607 12:57:30.555194 139945423042368 model_lib_v2.py:705] Step 3300 per-step time 0.645s\n","INFO:tensorflow:{'Loss/classification_loss': 0.7804395,\n"," 'Loss/localization_loss': 0.2636034,\n"," 'Loss/regularization_loss': 0.04434988,\n"," 'Loss/total_loss': 1.0883927,\n"," 'learning_rate': 0.07999857}\n","I0607 12:57:30.555564 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.7804395,\n"," 'Loss/localization_loss': 0.2636034,\n"," 'Loss/regularization_loss': 0.04434988,\n"," 'Loss/total_loss': 1.0883927,\n"," 'learning_rate': 0.07999857}\n","INFO:tensorflow:Step 3400 per-step time 0.643s\n","I0607 12:58:34.851937 139945423042368 model_lib_v2.py:705] Step 3400 per-step time 0.643s\n","INFO:tensorflow:{'Loss/classification_loss': 0.9019201,\n"," 'Loss/localization_loss': 0.5122605,\n"," 'Loss/regularization_loss': 0.04490159,\n"," 'Loss/total_loss': 1.4590821,\n"," 'learning_rate': 0.07999819}\n","I0607 12:58:34.852259 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.9019201,\n"," 'Loss/localization_loss': 0.5122605,\n"," 'Loss/regularization_loss': 0.04490159,\n"," 'Loss/total_loss': 1.4590821,\n"," 'learning_rate': 0.07999819}\n","INFO:tensorflow:Step 3500 per-step time 0.649s\n","I0607 12:59:39.723284 139945423042368 model_lib_v2.py:705] Step 3500 per-step time 0.649s\n","INFO:tensorflow:{'Loss/classification_loss': 0.81352895,\n"," 'Loss/localization_loss': 0.43021184,\n"," 'Loss/regularization_loss': 0.045340717,\n"," 'Loss/total_loss': 1.2890816,\n"," 'learning_rate': 0.07999776}\n","I0607 12:59:39.723720 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.81352895,\n"," 'Loss/localization_loss': 0.43021184,\n"," 'Loss/regularization_loss': 0.045340717,\n"," 'Loss/total_loss': 1.2890816,\n"," 'learning_rate': 0.07999776}\n","INFO:tensorflow:Step 3600 per-step time 0.645s\n","I0607 13:00:44.266448 139945423042368 model_lib_v2.py:705] Step 3600 per-step time 0.645s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6290514,\n"," 'Loss/localization_loss': 0.26666975,\n"," 'Loss/regularization_loss': 0.045828965,\n"," 'Loss/total_loss': 0.9415501,\n"," 'learning_rate': 0.0799973}\n","I0607 13:00:44.266905 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.6290514,\n"," 'Loss/localization_loss': 0.26666975,\n"," 'Loss/regularization_loss': 0.045828965,\n"," 'Loss/total_loss': 0.9415501,\n"," 'learning_rate': 0.0799973}\n","INFO:tensorflow:Step 3700 per-step time 0.643s\n","I0607 13:01:48.548937 139945423042368 model_lib_v2.py:705] Step 3700 per-step time 0.643s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8747301,\n"," 'Loss/localization_loss': 0.6493897,\n"," 'Loss/regularization_loss': 0.046272486,\n"," 'Loss/total_loss': 1.5703924,\n"," 'learning_rate': 0.07999679}\n","I0607 13:01:48.549254 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8747301,\n"," 'Loss/localization_loss': 0.6493897,\n"," 'Loss/regularization_loss': 0.046272486,\n"," 'Loss/total_loss': 1.5703924,\n"," 'learning_rate': 0.07999679}\n","INFO:tensorflow:Step 3800 per-step time 0.650s\n","I0607 13:02:53.537688 139945423042368 model_lib_v2.py:705] Step 3800 per-step time 0.650s\n","INFO:tensorflow:{'Loss/classification_loss': 0.76367486,\n"," 'Loss/localization_loss': 0.30842632,\n"," 'Loss/regularization_loss': 0.046713736,\n"," 'Loss/total_loss': 1.118815,\n"," 'learning_rate': 0.07999623}\n","I0607 13:02:53.538073 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.76367486,\n"," 'Loss/localization_loss': 0.30842632,\n"," 'Loss/regularization_loss': 0.046713736,\n"," 'Loss/total_loss': 1.118815,\n"," 'learning_rate': 0.07999623}\n","INFO:tensorflow:Step 3900 per-step time 0.644s\n","I0607 13:03:57.975220 139945423042368 model_lib_v2.py:705] Step 3900 per-step time 0.644s\n","INFO:tensorflow:{'Loss/classification_loss': 0.84034616,\n"," 'Loss/localization_loss': 0.49399388,\n"," 'Loss/regularization_loss': 0.047076695,\n"," 'Loss/total_loss': 1.3814168,\n"," 'learning_rate': 0.07999563}\n","I0607 13:03:57.975519 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.84034616,\n"," 'Loss/localization_loss': 0.49399388,\n"," 'Loss/regularization_loss': 0.047076695,\n"," 'Loss/total_loss': 1.3814168,\n"," 'learning_rate': 0.07999563}\n","INFO:tensorflow:Step 4000 per-step time 0.653s\n","I0607 13:05:03.261226 139945423042368 model_lib_v2.py:705] Step 4000 per-step time 0.653s\n","INFO:tensorflow:{'Loss/classification_loss': 1.0248415,\n"," 'Loss/localization_loss': 0.52628314,\n"," 'Loss/regularization_loss': 0.047469392,\n"," 'Loss/total_loss': 1.5985941,\n"," 'learning_rate': 0.079994984}\n","I0607 13:05:03.261669 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.0248415,\n"," 'Loss/localization_loss': 0.52628314,\n"," 'Loss/regularization_loss': 0.047469392,\n"," 'Loss/total_loss': 1.5985941,\n"," 'learning_rate': 0.079994984}\n","INFO:tensorflow:Step 4100 per-step time 0.664s\n","I0607 13:06:09.711359 139945423042368 model_lib_v2.py:705] Step 4100 per-step time 0.664s\n","INFO:tensorflow:{'Loss/classification_loss': 0.86564386,\n"," 'Loss/localization_loss': 0.44737092,\n"," 'Loss/regularization_loss': 0.047878776,\n"," 'Loss/total_loss': 1.3608935,\n"," 'learning_rate': 0.07999428}\n","I0607 13:06:09.711757 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.86564386,\n"," 'Loss/localization_loss': 0.44737092,\n"," 'Loss/regularization_loss': 0.047878776,\n"," 'Loss/total_loss': 1.3608935,\n"," 'learning_rate': 0.07999428}\n","INFO:tensorflow:Step 4200 per-step time 0.647s\n","I0607 13:07:14.404734 139945423042368 model_lib_v2.py:705] Step 4200 per-step time 0.647s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8516772,\n"," 'Loss/localization_loss': 0.3266796,\n"," 'Loss/regularization_loss': 0.04825882,\n"," 'Loss/total_loss': 1.2266155,\n"," 'learning_rate': 0.07999355}\n","I0607 13:07:14.405217 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8516772,\n"," 'Loss/localization_loss': 0.3266796,\n"," 'Loss/regularization_loss': 0.04825882,\n"," 'Loss/total_loss': 1.2266155,\n"," 'learning_rate': 0.07999355}\n","INFO:tensorflow:Step 4300 per-step time 0.645s\n","I0607 13:08:18.941736 139945423042368 model_lib_v2.py:705] Step 4300 per-step time 0.645s\n","INFO:tensorflow:{'Loss/classification_loss': 0.7413447,\n"," 'Loss/localization_loss': 0.26274624,\n"," 'Loss/regularization_loss': 0.048688076,\n"," 'Loss/total_loss': 1.052779,\n"," 'learning_rate': 0.07999277}\n","I0607 13:08:18.942065 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.7413447,\n"," 'Loss/localization_loss': 0.26274624,\n"," 'Loss/regularization_loss': 0.048688076,\n"," 'Loss/total_loss': 1.052779,\n"," 'learning_rate': 0.07999277}\n","INFO:tensorflow:Step 4400 per-step time 0.653s\n","I0607 13:09:24.281984 139945423042368 model_lib_v2.py:705] Step 4400 per-step time 0.653s\n","INFO:tensorflow:{'Loss/classification_loss': 0.5358435,\n"," 'Loss/localization_loss': 0.18251477,\n"," 'Loss/regularization_loss': 0.049142856,\n"," 'Loss/total_loss': 0.7675011,\n"," 'learning_rate': 0.07999195}\n","I0607 13:09:24.282356 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.5358435,\n"," 'Loss/localization_loss': 0.18251477,\n"," 'Loss/regularization_loss': 0.049142856,\n"," 'Loss/total_loss': 0.7675011,\n"," 'learning_rate': 0.07999195}\n","INFO:tensorflow:Step 4500 per-step time 0.644s\n","I0607 13:10:28.660369 139945423042368 model_lib_v2.py:705] Step 4500 per-step time 0.644s\n","INFO:tensorflow:{'Loss/classification_loss': 0.9524774,\n"," 'Loss/localization_loss': 0.42025852,\n"," 'Loss/regularization_loss': 0.049587943,\n"," 'Loss/total_loss': 1.4223238,\n"," 'learning_rate': 0.07999108}\n","I0607 13:10:28.660685 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.9524774,\n"," 'Loss/localization_loss': 0.42025852,\n"," 'Loss/regularization_loss': 0.049587943,\n"," 'Loss/total_loss': 1.4223238,\n"," 'learning_rate': 0.07999108}\n","INFO:tensorflow:Step 4600 per-step time 0.648s\n","I0607 13:11:33.454213 139945423042368 model_lib_v2.py:705] Step 4600 per-step time 0.648s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6705426,\n"," 'Loss/localization_loss': 0.1443922,\n"," 'Loss/regularization_loss': 0.049867056,\n"," 'Loss/total_loss': 0.8648018,\n"," 'learning_rate': 0.07999016}\n","I0607 13:11:33.454654 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.6705426,\n"," 'Loss/localization_loss': 0.1443922,\n"," 'Loss/regularization_loss': 0.049867056,\n"," 'Loss/total_loss': 0.8648018,\n"," 'learning_rate': 0.07999016}\n","INFO:tensorflow:Step 4700 per-step time 0.647s\n","I0607 13:12:38.125413 139945423042368 model_lib_v2.py:705] Step 4700 per-step time 0.647s\n","INFO:tensorflow:{'Loss/classification_loss': 0.56036997,\n"," 'Loss/localization_loss': 0.48568755,\n"," 'Loss/regularization_loss': 0.050264932,\n"," 'Loss/total_loss': 1.0963224,\n"," 'learning_rate': 0.0799892}\n","I0607 13:12:38.125845 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.56036997,\n"," 'Loss/localization_loss': 0.48568755,\n"," 'Loss/regularization_loss': 0.050264932,\n"," 'Loss/total_loss': 1.0963224,\n"," 'learning_rate': 0.0799892}\n","INFO:tensorflow:Step 4800 per-step time 0.646s\n","I0607 13:13:42.699577 139945423042368 model_lib_v2.py:705] Step 4800 per-step time 0.646s\n","INFO:tensorflow:{'Loss/classification_loss': 0.9528937,\n"," 'Loss/localization_loss': 0.31681314,\n"," 'Loss/regularization_loss': 0.050693914,\n"," 'Loss/total_loss': 1.3204007,\n"," 'learning_rate': 0.079988204}\n","I0607 13:13:42.699987 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.9528937,\n"," 'Loss/localization_loss': 0.31681314,\n"," 'Loss/regularization_loss': 0.050693914,\n"," 'Loss/total_loss': 1.3204007,\n"," 'learning_rate': 0.079988204}\n","INFO:tensorflow:Step 4900 per-step time 0.651s\n","I0607 13:14:47.745210 139945423042368 model_lib_v2.py:705] Step 4900 per-step time 0.651s\n","INFO:tensorflow:{'Loss/classification_loss': 0.9102066,\n"," 'Loss/localization_loss': 0.48141152,\n"," 'Loss/regularization_loss': 0.05103087,\n"," 'Loss/total_loss': 1.442649,\n"," 'learning_rate': 0.07998715}\n","I0607 13:14:47.745590 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.9102066,\n"," 'Loss/localization_loss': 0.48141152,\n"," 'Loss/regularization_loss': 0.05103087,\n"," 'Loss/total_loss': 1.442649,\n"," 'learning_rate': 0.07998715}\n","INFO:tensorflow:Step 5000 per-step time 0.644s\n","I0607 13:15:52.100149 139945423042368 model_lib_v2.py:705] Step 5000 per-step time 0.644s\n","INFO:tensorflow:{'Loss/classification_loss': 0.4673715,\n"," 'Loss/localization_loss': 0.27861813,\n"," 'Loss/regularization_loss': 0.051417388,\n"," 'Loss/total_loss': 0.79740703,\n"," 'learning_rate': 0.07998606}\n","I0607 13:15:52.100453 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.4673715,\n"," 'Loss/localization_loss': 0.27861813,\n"," 'Loss/regularization_loss': 0.051417388,\n"," 'Loss/total_loss': 0.79740703,\n"," 'learning_rate': 0.07998606}\n","INFO:tensorflow:Step 5100 per-step time 0.661s\n","I0607 13:16:58.205333 139945423042368 model_lib_v2.py:705] Step 5100 per-step time 0.661s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8381007,\n"," 'Loss/localization_loss': 0.3703334,\n"," 'Loss/regularization_loss': 0.051808685,\n"," 'Loss/total_loss': 1.2602428,\n"," 'learning_rate': 0.07998492}\n","I0607 13:16:58.205714 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8381007,\n"," 'Loss/localization_loss': 0.3703334,\n"," 'Loss/regularization_loss': 0.051808685,\n"," 'Loss/total_loss': 1.2602428,\n"," 'learning_rate': 0.07998492}\n","INFO:tensorflow:Step 5200 per-step time 0.642s\n","I0607 13:18:02.387749 139945423042368 model_lib_v2.py:705] Step 5200 per-step time 0.642s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6883897,\n"," 'Loss/localization_loss': 0.282189,\n"," 'Loss/regularization_loss': 0.05222557,\n"," 'Loss/total_loss': 1.0228043,\n"," 'learning_rate': 0.07998374}\n","I0607 13:18:02.388131 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.6883897,\n"," 'Loss/localization_loss': 0.282189,\n"," 'Loss/regularization_loss': 0.05222557,\n"," 'Loss/total_loss': 1.0228043,\n"," 'learning_rate': 0.07998374}\n","INFO:tensorflow:Step 5300 per-step time 0.652s\n","I0607 13:19:07.602092 139945423042368 model_lib_v2.py:705] Step 5300 per-step time 0.652s\n","INFO:tensorflow:{'Loss/classification_loss': 0.72843057,\n"," 'Loss/localization_loss': 0.20179148,\n"," 'Loss/regularization_loss': 0.052575216,\n"," 'Loss/total_loss': 0.98279727,\n"," 'learning_rate': 0.07998252}\n","I0607 13:19:07.602625 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.72843057,\n"," 'Loss/localization_loss': 0.20179148,\n"," 'Loss/regularization_loss': 0.052575216,\n"," 'Loss/total_loss': 0.98279727,\n"," 'learning_rate': 0.07998252}\n","INFO:tensorflow:Step 5400 per-step time 0.650s\n","I0607 13:20:12.629031 139945423042368 model_lib_v2.py:705] Step 5400 per-step time 0.650s\n","INFO:tensorflow:{'Loss/classification_loss': 0.7346394,\n"," 'Loss/localization_loss': 0.47025862,\n"," 'Loss/regularization_loss': 0.052885793,\n"," 'Loss/total_loss': 1.2577838,\n"," 'learning_rate': 0.079981245}\n","I0607 13:20:12.629385 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.7346394,\n"," 'Loss/localization_loss': 0.47025862,\n"," 'Loss/regularization_loss': 0.052885793,\n"," 'Loss/total_loss': 1.2577838,\n"," 'learning_rate': 0.079981245}\n","INFO:tensorflow:Step 5500 per-step time 0.649s\n","I0607 13:21:17.490872 139945423042368 model_lib_v2.py:705] Step 5500 per-step time 0.649s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6511509,\n"," 'Loss/localization_loss': 0.2731064,\n"," 'Loss/regularization_loss': 0.053257048,\n"," 'Loss/total_loss': 0.9775143,\n"," 'learning_rate': 0.07997993}\n","I0607 13:21:17.491247 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.6511509,\n"," 'Loss/localization_loss': 0.2731064,\n"," 'Loss/regularization_loss': 0.053257048,\n"," 'Loss/total_loss': 0.9775143,\n"," 'learning_rate': 0.07997993}\n","INFO:tensorflow:Step 5600 per-step time 0.648s\n","I0607 13:22:22.327658 139945423042368 model_lib_v2.py:705] Step 5600 per-step time 0.648s\n","INFO:tensorflow:{'Loss/classification_loss': 0.7222688,\n"," 'Loss/localization_loss': 0.34290275,\n"," 'Loss/regularization_loss': 0.053669855,\n"," 'Loss/total_loss': 1.1188414,\n"," 'learning_rate': 0.07997857}\n","I0607 13:22:22.328004 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.7222688,\n"," 'Loss/localization_loss': 0.34290275,\n"," 'Loss/regularization_loss': 0.053669855,\n"," 'Loss/total_loss': 1.1188414,\n"," 'learning_rate': 0.07997857}\n","INFO:tensorflow:Step 5700 per-step time 0.648s\n","I0607 13:23:27.144051 139945423042368 model_lib_v2.py:705] Step 5700 per-step time 0.648s\n","INFO:tensorflow:{'Loss/classification_loss': 1.0252568,\n"," 'Loss/localization_loss': 0.48774096,\n"," 'Loss/regularization_loss': 0.05407525,\n"," 'Loss/total_loss': 1.567073,\n"," 'learning_rate': 0.07997716}\n","I0607 13:23:27.144512 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 1.0252568,\n"," 'Loss/localization_loss': 0.48774096,\n"," 'Loss/regularization_loss': 0.05407525,\n"," 'Loss/total_loss': 1.567073,\n"," 'learning_rate': 0.07997716}\n","INFO:tensorflow:Step 5800 per-step time 0.654s\n","I0607 13:24:32.564574 139945423042368 model_lib_v2.py:705] Step 5800 per-step time 0.654s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8199005,\n"," 'Loss/localization_loss': 0.4399655,\n"," 'Loss/regularization_loss': 0.054446697,\n"," 'Loss/total_loss': 1.3143127,\n"," 'learning_rate': 0.07997571}\n","I0607 13:24:32.564997 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8199005,\n"," 'Loss/localization_loss': 0.4399655,\n"," 'Loss/regularization_loss': 0.054446697,\n"," 'Loss/total_loss': 1.3143127,\n"," 'learning_rate': 0.07997571}\n","INFO:tensorflow:Step 5900 per-step time 0.646s\n","I0607 13:25:37.151991 139945423042368 model_lib_v2.py:705] Step 5900 per-step time 0.646s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8155319,\n"," 'Loss/localization_loss': 0.41098872,\n"," 'Loss/regularization_loss': 0.054809254,\n"," 'Loss/total_loss': 1.2813299,\n"," 'learning_rate': 0.07997422}\n","I0607 13:25:37.152428 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.8155319,\n"," 'Loss/localization_loss': 0.41098872,\n"," 'Loss/regularization_loss': 0.054809254,\n"," 'Loss/total_loss': 1.2813299,\n"," 'learning_rate': 0.07997422}\n","INFO:tensorflow:Step 6000 per-step time 0.648s\n","I0607 13:26:41.949980 139945423042368 model_lib_v2.py:705] Step 6000 per-step time 0.648s\n","INFO:tensorflow:{'Loss/classification_loss': 0.86310095,\n"," 'Loss/localization_loss': 0.27131984,\n"," 'Loss/regularization_loss': 0.05516101,\n"," 'Loss/total_loss': 1.1895818,\n"," 'learning_rate': 0.07997268}\n","I0607 13:26:41.950393 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.86310095,\n"," 'Loss/localization_loss': 0.27131984,\n"," 'Loss/regularization_loss': 0.05516101,\n"," 'Loss/total_loss': 1.1895818,\n"," 'learning_rate': 0.07997268}\n","INFO:tensorflow:Step 6100 per-step time 0.672s\n","I0607 13:27:49.137491 139945423042368 model_lib_v2.py:705] Step 6100 per-step time 0.672s\n","INFO:tensorflow:{'Loss/classification_loss': 0.71583366,\n"," 'Loss/localization_loss': 0.28343505,\n"," 'Loss/regularization_loss': 0.055441312,\n"," 'Loss/total_loss': 1.05471,\n"," 'learning_rate': 0.0799711}\n","I0607 13:27:49.137863 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.71583366,\n"," 'Loss/localization_loss': 0.28343505,\n"," 'Loss/regularization_loss': 0.055441312,\n"," 'Loss/total_loss': 1.05471,\n"," 'learning_rate': 0.0799711}\n","INFO:tensorflow:Step 6200 per-step time 0.651s\n","I0607 13:28:54.291133 139945423042368 model_lib_v2.py:705] Step 6200 per-step time 0.651s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6858346,\n"," 'Loss/localization_loss': 0.33470818,\n"," 'Loss/regularization_loss': 0.055713244,\n"," 'Loss/total_loss': 1.076256,\n"," 'learning_rate': 0.07996947}\n","I0607 13:28:54.291539 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.6858346,\n"," 'Loss/localization_loss': 0.33470818,\n"," 'Loss/regularization_loss': 0.055713244,\n"," 'Loss/total_loss': 1.076256,\n"," 'learning_rate': 0.07996947}\n","INFO:tensorflow:Step 6300 per-step time 0.652s\n","I0607 13:29:59.449971 139945423042368 model_lib_v2.py:705] Step 6300 per-step time 0.652s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6681945,\n"," 'Loss/localization_loss': 0.41558853,\n"," 'Loss/regularization_loss': 0.056084953,\n"," 'Loss/total_loss': 1.1398679,\n"," 'learning_rate': 0.0799678}\n","I0607 13:29:59.450320 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.6681945,\n"," 'Loss/localization_loss': 0.41558853,\n"," 'Loss/regularization_loss': 0.056084953,\n"," 'Loss/total_loss': 1.1398679,\n"," 'learning_rate': 0.0799678}\n","INFO:tensorflow:Step 6400 per-step time 0.645s\n","I0607 13:31:03.990392 139945423042368 model_lib_v2.py:705] Step 6400 per-step time 0.645s\n","INFO:tensorflow:{'Loss/classification_loss': 0.88960874,\n"," 'Loss/localization_loss': 0.24495263,\n"," 'Loss/regularization_loss': 0.0564694,\n"," 'Loss/total_loss': 1.1910309,\n"," 'learning_rate': 0.07996608}\n","I0607 13:31:03.990863 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.88960874,\n"," 'Loss/localization_loss': 0.24495263,\n"," 'Loss/regularization_loss': 0.0564694,\n"," 'Loss/total_loss': 1.1910309,\n"," 'learning_rate': 0.07996608}\n","INFO:tensorflow:Step 6500 per-step time 0.645s\n","I0607 13:32:08.487378 139945423042368 model_lib_v2.py:705] Step 6500 per-step time 0.645s\n","INFO:tensorflow:{'Loss/classification_loss': 0.79813695,\n"," 'Loss/localization_loss': 0.49375778,\n"," 'Loss/regularization_loss': 0.056802835,\n"," 'Loss/total_loss': 1.3486975,\n"," 'learning_rate': 0.079964325}\n","I0607 13:32:08.487766 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.79813695,\n"," 'Loss/localization_loss': 0.49375778,\n"," 'Loss/regularization_loss': 0.056802835,\n"," 'Loss/total_loss': 1.3486975,\n"," 'learning_rate': 0.079964325}\n","INFO:tensorflow:Step 6600 per-step time 0.647s\n","I0607 13:33:13.148044 139945423042368 model_lib_v2.py:705] Step 6600 per-step time 0.647s\n","INFO:tensorflow:{'Loss/classification_loss': 0.56580627,\n"," 'Loss/localization_loss': 0.3545789,\n"," 'Loss/regularization_loss': 0.05708969,\n"," 'Loss/total_loss': 0.97747487,\n"," 'learning_rate': 0.079962514}\n","I0607 13:33:13.148488 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.56580627,\n"," 'Loss/localization_loss': 0.3545789,\n"," 'Loss/regularization_loss': 0.05708969,\n"," 'Loss/total_loss': 0.97747487,\n"," 'learning_rate': 0.079962514}\n","INFO:tensorflow:Step 6700 per-step time 0.651s\n","I0607 13:34:18.266005 139945423042368 model_lib_v2.py:705] Step 6700 per-step time 0.651s\n","INFO:tensorflow:{'Loss/classification_loss': 0.69982886,\n"," 'Loss/localization_loss': 0.36244056,\n"," 'Loss/regularization_loss': 0.05750662,\n"," 'Loss/total_loss': 1.119776,\n"," 'learning_rate': 0.07996067}\n","I0607 13:34:18.266308 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.69982886,\n"," 'Loss/localization_loss': 0.36244056,\n"," 'Loss/regularization_loss': 0.05750662,\n"," 'Loss/total_loss': 1.119776,\n"," 'learning_rate': 0.07996067}\n","INFO:tensorflow:Step 6800 per-step time 0.648s\n","I0607 13:35:23.045924 139945423042368 model_lib_v2.py:705] Step 6800 per-step time 0.648s\n","INFO:tensorflow:{'Loss/classification_loss': 0.83306414,\n"," 'Loss/localization_loss': 0.47098833,\n"," 'Loss/regularization_loss': 0.057826065,\n"," 'Loss/total_loss': 1.3618785,\n"," 'learning_rate': 0.079958774}\n","I0607 13:35:23.046285 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.83306414,\n"," 'Loss/localization_loss': 0.47098833,\n"," 'Loss/regularization_loss': 0.057826065,\n"," 'Loss/total_loss': 1.3618785,\n"," 'learning_rate': 0.079958774}\n","INFO:tensorflow:Step 6900 per-step time 0.647s\n","I0607 13:36:27.776273 139945423042368 model_lib_v2.py:705] Step 6900 per-step time 0.647s\n","INFO:tensorflow:{'Loss/classification_loss': 0.977838,\n"," 'Loss/localization_loss': 0.5391273,\n"," 'Loss/regularization_loss': 0.058066774,\n"," 'Loss/total_loss': 1.575032,\n"," 'learning_rate': 0.07995682}\n","I0607 13:36:27.776590 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.977838,\n"," 'Loss/localization_loss': 0.5391273,\n"," 'Loss/regularization_loss': 0.058066774,\n"," 'Loss/total_loss': 1.575032,\n"," 'learning_rate': 0.07995682}\n","INFO:tensorflow:Step 7000 per-step time 0.643s\n","I0607 13:37:32.075062 139945423042368 model_lib_v2.py:705] Step 7000 per-step time 0.643s\n","INFO:tensorflow:{'Loss/classification_loss': 0.76830107,\n"," 'Loss/localization_loss': 0.33332893,\n"," 'Loss/regularization_loss': 0.058386307,\n"," 'Loss/total_loss': 1.1600163,\n"," 'learning_rate': 0.07995484}\n","I0607 13:37:32.075503 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.76830107,\n"," 'Loss/localization_loss': 0.33332893,\n"," 'Loss/regularization_loss': 0.058386307,\n"," 'Loss/total_loss': 1.1600163,\n"," 'learning_rate': 0.07995484}\n","INFO:tensorflow:Step 7100 per-step time 0.671s\n","I0607 13:38:39.225735 139945423042368 model_lib_v2.py:705] Step 7100 per-step time 0.671s\n","INFO:tensorflow:{'Loss/classification_loss': 0.44930312,\n"," 'Loss/localization_loss': 0.22331426,\n"," 'Loss/regularization_loss': 0.058745362,\n"," 'Loss/total_loss': 0.7313627,\n"," 'learning_rate': 0.07995281}\n","I0607 13:38:39.226310 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.44930312,\n"," 'Loss/localization_loss': 0.22331426,\n"," 'Loss/regularization_loss': 0.058745362,\n"," 'Loss/total_loss': 0.7313627,\n"," 'learning_rate': 0.07995281}\n","INFO:tensorflow:Step 7200 per-step time 0.649s\n","I0607 13:39:44.162322 139945423042368 model_lib_v2.py:705] Step 7200 per-step time 0.649s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6629198,\n"," 'Loss/localization_loss': 0.39832887,\n"," 'Loss/regularization_loss': 0.059057448,\n"," 'Loss/total_loss': 1.1203061,\n"," 'learning_rate': 0.07995074}\n","I0607 13:39:44.162761 139945423042368 model_lib_v2.py:708] {'Loss/classification_loss': 0.6629198,\n"," 'Loss/localization_loss': 0.39832887,\n"," 'Loss/regularization_loss': 0.059057448,\n"," 'Loss/total_loss': 1.1203061,\n"," 'learning_rate': 0.07995074}\n"]}],"source":["# Run training!\n","!python /content/models/research/object_detection/model_main_tf2.py \\\n","    --pipeline_config_path={pipeline_file} \\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --sample_1_of_n_eval_examples=1"]},{"cell_type":"markdown","metadata":{"id":"WHxbX4ZpzXIv"},"source":["If you want to stop training early, just click Stop a couple times or right-click on the code block and select \"Interrupt Execution\". Otherwise, training will stop by itself once it reaches the specified number of training steps.\n"]},{"cell_type":"markdown","metadata":{"id":"kPg8oMnQDYKl"},"source":["# 6.&nbsp;Convert Model to TensorFlow Lite"]},{"cell_type":"markdown","metadata":{"id":"spQXdq8Y63pj"},"source":["Alright! Our model is all trained up and ready to be used for detecting objects. First, we need to export the model graph (a file that contains information about the architecture and weights) to a TensorFlow Lite-compatible format. We'll do this using the `export_tflite_graph_tf2.py` script."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaUU8tBlHifd"},"outputs":[],"source":["# Make a directory to store the trained TFLite model\n","!mkdir /content/custom_model_lite\n","output_directory = '/content/custom_model_lite'\n","\n","# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n","last_model_path = '/content/training'\n","\n","!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n","    --trained_checkpoint_dir {last_model_path} \\\n","    --output_directory {output_directory} \\\n","    --pipeline_config_path {pipeline_file}\n"]},{"cell_type":"markdown","metadata":{"id":"z_NuapO2VROu"},"source":["Next, we'll take the exported graph and use the `TFLiteConverter` module to convert it to `.tflite` FlatBuffer format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsE_uVjlsz3u"},"outputs":[],"source":["# Convert exported graph file into TFLite model file\n","import tensorflow as tf\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n","tflite_model = converter.convert()\n","\n","with open('/content/custom_model_lite/detect.tflite', 'wb') as f:\n","  f.write(tflite_model)"]},{"cell_type":"markdown","metadata":{"id":"RDQrtQhvC3oG"},"source":["# 7.&nbsp;Test TensorFlow Lite Model and Calculate mAP"]},{"cell_type":"markdown","metadata":{"id":"vtSmUZcxIAvt"},"source":["We've trained our custom model and converted it to TFLite format. But how well does it actually perform at detecting objects in images? This is where the images we set aside in the **test** folder come in. The model never saw any test images during training, so its performance on these images should be representative of how it will perform on new images from the field.\n","\n","### 7.1 Inference test images\n","The following code defines a function to run inference on test images. It loads the images, loads the model and labelmap, runs the model on each image, and displays the result. It also optionally saves detection results as text files so we can use them to calculate model mAP score.\n","\n","This code is based off the [TFLite_detection_image.py](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py) script from my [TensorFlow Lite Object Detection repository on GitHub](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi); feel free to use it as a starting point for your own application."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4WtI8i5K96w"},"outputs":[],"source":["# Script to run custom TFLite model on test images to detect objects\n","# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n","\n","# Import packages\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import glob\n","import random\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","### Define function for inferencing with TFLite model and displaying results\n","\n","def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n","\n","  # Grab filenames of all images in test folder\n","  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n","\n","  # Load the label map into memory\n","  with open(lblpath, 'r') as f:\n","      labels = [line.strip() for line in f.readlines()]\n","\n","  # Load the Tensorflow Lite model into memory\n","  interpreter = Interpreter(model_path=modelpath)\n","  interpreter.allocate_tensors()\n","\n","  # Get model details\n","  input_details = interpreter.get_input_details()\n","  output_details = interpreter.get_output_details()\n","  height = input_details[0]['shape'][1]\n","  width = input_details[0]['shape'][2]\n","\n","  float_input = (input_details[0]['dtype'] == np.float32)\n","\n","  input_mean = 127.5\n","  input_std = 127.5\n","\n","  # Randomly select test images\n","  images_to_test = random.sample(images, num_test_images)\n","\n","  # Loop over every image and perform detection\n","  for image_path in images_to_test:\n","\n","      # Load image and resize to expected shape [1xHxWx3]\n","      image = cv2.imread(image_path)\n","      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","      imH, imW, _ = image.shape\n","      image_resized = cv2.resize(image_rgb, (width, height))\n","      input_data = np.expand_dims(image_resized, axis=0)\n","\n","      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n","      if float_input:\n","          input_data = (np.float32(input_data) - input_mean) / input_std\n","\n","      # Perform the actual detection by running the model with the image as input\n","      interpreter.set_tensor(input_details[0]['index'],input_data)\n","      interpreter.invoke()\n","\n","      # Retrieve detection results\n","      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n","      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n","      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n","\n","      detections = []\n","\n","      # Loop over all detections and draw detection box if confidence is above minimum threshold\n","      for i in range(len(scores)):\n","          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n","\n","              # Get bounding box coordinates and draw box\n","              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n","              ymin = int(max(1,(boxes[i][0] * imH)))\n","              xmin = int(max(1,(boxes[i][1] * imW)))\n","              ymax = int(min(imH,(boxes[i][2] * imH)))\n","              xmax = int(min(imW,(boxes[i][3] * imW)))\n","\n","              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n","\n","              # Draw label\n","              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n","              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n","              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n","              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n","              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n","              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n","\n","              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n","\n","\n","      # All the results have been drawn on the image, now display the image\n","      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n","        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","        plt.figure(figsize=(12,16))\n","        plt.imshow(image)\n","        plt.show()\n","\n","      # Save detection results in .txt files (for calculating mAP)\n","      elif txt_only == True:\n","\n","        # Get filenames and paths\n","        image_fn = os.path.basename(image_path)\n","        base_fn, ext = os.path.splitext(image_fn)\n","        txt_result_fn = base_fn +'.txt'\n","        txt_savepath = os.path.join(savepath, txt_result_fn)\n","\n","        # Write results to text file\n","        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n","        with open(txt_savepath,'w') as f:\n","            for detection in detections:\n","                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n","\n","  return"]},{"cell_type":"markdown","metadata":{"id":"-CJI4A0f_zqz"},"source":["The next block sets the paths to the test images and models and then runs the inferencing function. If you want to use more than 10 images, change the `images_to_test` variable. Click play to run inferencing!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1GM0_N9FOAuQ_dHEOosCG6UHYx9PUqX8U"},"executionInfo":{"elapsed":88703,"status":"ok","timestamp":1686135812638,"user":{"displayName":"Ahmed Boussihmed","userId":"16936910001719053027"},"user_tz":-60},"id":"6t8CMarqBqP9","outputId":"dfceb57c-93f8-4d64-9132-713bf246f665"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Set up variables for running user's model\n","PATH_TO_IMAGES='/content/drive/MyDrive/OOD/test'   # Path to test images folder\n","PATH_TO_MODEL='/content/drive/MyDrive/ssd-mobilenet-v2-320x320/custom_model_lite/ssd-mobilenet-v2-320x320.tflite'   # Path to .tflite model file\n","PATH_TO_LABELS='/content/drive/MyDrive/OOD/labelmap.txt'   # Path to labelmap.txt file\n","min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n","images_to_test = 20   # Number of images to run detection on\n","\n","# Run inferencing function!\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"]},{"cell_type":"markdown","metadata":{"id":"N_ckqeWqBF0P"},"source":["### 7.2 Calculate mAP\n","Now we have a visual sense of how our model performs on test images, but how can we quantitatively measure its accuracy?\n","\n","One popular methord for measuring object detection model accuracy is \"mean average precision\" (mAP). Basically, the higher the mAP score, the better your model is at detecting objects in images. To learn more about mAP, read through this [article from Roboflow](https://blog.roboflow.com/mean-average-precision/).\n","\n","We'll use the mAP calculator tool at https://github.com/Cartucho/mAP to determine our model's mAP score. First, we need to clone the repository and remove its existing example data. We'll also download a script I wrote for interfacing with the calculator."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1646,"status":"ok","timestamp":1686055822799,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"JlWarXEZDUqS","outputId":"bf28334f-4f6c-4d20-d0f2-5782af38ff90"},"outputs":[{"name":"stderr","output_type":"stream","text":["rm: cannot remove 'input/ground-truth/*': No such file or directory\n","rm: cannot remove 'input/images-optional/*': No such file or directory\n","--2023-06-06 12:50:21--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5397 (5.3K) [text/plain]\n","Saving to: ‘calculate_map_cartucho.py.1’\n","\n","     0K .....                                                 100% 4.98M=0.001s\n","\n","2023-06-06 12:50:21 (4.98 MB/s) - ‘calculate_map_cartucho.py.1’ saved [5397/5397]\n","\n"]}],"source":["%%bash\n","git clone https://github.com/Cartucho/mAP /content/drive/MyDrive/mAP\n","cd /content/drive/MyDrive/mAP\n","rm input/detection-results/*\n","rm input/ground-truth/*\n","rm input/images-optional/*\n","wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"]},{"cell_type":"markdown","metadata":{"id":"qn22nGGqH5T6"},"source":["Next, we'll copy the images and annotation data from the **test** folder to the appropriate folders inside the cloned repository. These will be used as the \"ground truth data\" that our model's detection results will be compared to.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5szFfVxwI3wT"},"outputs":[],"source":["!cp /content/drive/MyDrive/OOD/test/* /content/drive/MyDrive/mAP/input/images-optional # Copy images and xml files\n","!mv /content/drive/MyDrive/mAP/input/images-optional/*.xml /content/drive/MyDrive/mAP/input/ground-truth/  # Move xml files to the appropriate folder"]},{"cell_type":"markdown","metadata":{"id":"u6aro817DGzx"},"source":["The calculator tool expects annotation data in a format that's different from the Pascal VOC .xml file format we're using. Fortunately, it provides an easy script, `convert_gt_xml.py`, for converting to the expected .txt format.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9542,"status":"ok","timestamp":1686055900704,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"qdjtOUDnK2AA","outputId":"13e126de-ca66-4f69-9c2c-475f2aa2c6fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Conversion completed!\n"]}],"source":["!python /content/drive/MyDrive/mAP/scripts/extra/convert_gt_xml.py"]},{"cell_type":"markdown","metadata":{"id":"mnIUacAlLP0B"},"source":["Okay, we've set up the ground truth data, but now we need actual detection results from our model. The detection results will be compared to the ground truth data to calculate the model's accuracy in mAP.\n","\n","The inference function we defined in Step 7.1 can be used to generate detection data for all the images in the **test** folder. We'll use it the same as before, except this time we'll tell it to save detection results into the `detection-results` folder.\n","\n","Click Play to run the following code block!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szzHFAhsMNFF","outputId":"7661a67f-652d-42b1-fd7b-de1f7146dcca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting inference on 1000 images...\n"]}],"source":["# Set up variables for running inference, this time to get detection results saved as .txt files\n","PATH_TO_IMAGES='/content/drive/MyDrive/OOD/test'   # Path to test images folder\n","PATH_TO_MODEL='/content/drive/MyDrive/ssd-mobilenet-v2-320x320/custom_model_lite/ssd-mobilenet-v2-320x320.tflite'   # Path to .tflite model file\n","PATH_TO_LABELS='/content/drive/MyDrive/OOD/labelmap.txt'   # Path to labelmap.txt file\n","PATH_TO_RESULTS='/content/drive/MyDrive/mAP/input/detection-results' # Folder to save detection results in\n","min_conf_threshold=0.1   # Confidence threshold\n","\n","# Use all the images in the test folder\n","image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n","images_to_test = min(1000, len(image_list)) # If there are more than 500 images in the folder, just use 500\n","\n","# Tell function to just save results and not display images\n","txt_only = True\n","\n","# Run inferencing function!\n","print('Starting inference on %d images...' % images_to_test)\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n","print('Finished inferencing!')"]},{"cell_type":"markdown","metadata":{"id":"e_QRnTqNPX4z"},"source":["Finally, let's calculate mAP! One popular style for reporting mAP is the COCO metric for mAP @ 0.50:0.95. Basically, this means that mAP is calculated at several IoU thresholds between 0.50 and 0.95, and then the result from each threshold is averaged to get a final mAP score. [Learn more here!](https://blog.roboflow.com/mean-average-precision/)\n","\n","I wrote a script to run the calculator tool at each IoU threshold, average the results, and report the final accuracy score. It reports mAP for each class and overall mAP. Click Play on the following two blocks to calculate mAP!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":469093,"status":"ok","timestamp":1686061124471,"user":{"displayName":"Ahmed Boussihmed","userId":"16936910001719053027"},"user_tz":-60},"id":"3DkjpIBARTQ7","outputId":"6d5eaefb-ae56-44b8-8f21-6e57735f1b60"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/mAP\n","Calculating mAP at 0.50 IoU threshold...\n","39.23% = bench AP \n","47.76% = bicycle AP \n","60.60% = bus AP \n","83.38% = bus_stop AP \n","48.24% = car AP \n","19.45% = crutch AP \n","30.39% = curb AP \n","55.64% = dog AP \n","67.54% = fire_hydrant AP \n","52.01% = motorcycle AP \n","11.94% = person AP \n","9.12% = pole AP \n","68.99% = spherical_roadblock AP \n","58.03% = stairs AP \n","77.55% = stop_sign AP \n","5.53% = street_light AP \n","17.60% = traffic_light AP \n","70.09% = train AP \n","11.43% = tree AP \n","42.91% = truck AP \n","40.26% = warning_column AP \n","67.72% = waste_container AP \n","mAP = 44.79%\n","Calculating mAP at 0.55 IoU threshold...\n","35.48% = bench AP \n","47.76% = bicycle AP \n","60.60% = bus AP \n","83.38% = bus_stop AP \n","46.81% = car AP \n","18.74% = crutch AP \n","29.33% = curb AP \n","53.18% = dog AP \n","65.75% = fire_hydrant AP \n","48.29% = motorcycle AP \n","10.25% = person AP \n","7.00% = pole AP \n","67.23% = spherical_roadblock AP \n","53.20% = stairs AP \n","77.55% = stop_sign AP \n","4.07% = street_light AP \n","15.05% = traffic_light AP \n","66.34% = train AP \n","10.45% = tree AP \n","41.14% = truck AP \n","38.13% = warning_column AP \n","65.70% = waste_container AP \n","mAP = 42.97%\n","Calculating mAP at 0.60 IoU threshold...\n","34.31% = bench AP \n","43.64% = bicycle AP \n","60.56% = bus AP \n","81.05% = bus_stop AP \n","45.73% = car AP \n","16.57% = crutch AP \n","28.24% = curb AP \n","43.02% = dog AP \n","65.11% = fire_hydrant AP \n","44.23% = motorcycle AP \n","9.14% = person AP \n","5.45% = pole AP \n","62.16% = spherical_roadblock AP \n","50.15% = stairs AP \n","76.34% = stop_sign AP \n","2.90% = street_light AP \n","11.67% = traffic_light AP \n","65.44% = train AP \n","9.49% = tree AP \n","40.22% = truck AP \n","34.11% = warning_column AP \n","62.71% = waste_container AP \n","mAP = 40.56%\n","Calculating mAP at 0.65 IoU threshold...\n","30.79% = bench AP \n","40.68% = bicycle AP \n","60.56% = bus AP \n","81.05% = bus_stop AP \n","43.80% = car AP \n","9.50% = crutch AP \n","26.03% = curb AP \n","41.19% = dog AP \n","63.72% = fire_hydrant AP \n","43.36% = motorcycle AP \n","6.86% = person AP \n","2.84% = pole AP \n","55.21% = spherical_roadblock AP \n","40.22% = stairs AP \n","74.14% = stop_sign AP \n","1.29% = street_light AP \n","9.30% = traffic_light AP \n","65.44% = train AP \n","7.07% = tree AP \n","40.22% = truck AP \n","25.76% = warning_column AP \n","62.35% = waste_container AP \n","mAP = 37.79%\n","Calculating mAP at 0.70 IoU threshold...\n","28.17% = bench AP \n","35.95% = bicycle AP \n","59.65% = bus AP \n","75.43% = bus_stop AP \n","39.81% = car AP \n","1.72% = crutch AP \n","21.39% = curb AP \n","38.99% = dog AP \n","54.36% = fire_hydrant AP \n","41.37% = motorcycle AP \n","4.50% = person AP \n","0.61% = pole AP \n","48.15% = spherical_roadblock AP \n","34.81% = stairs AP \n","73.43% = stop_sign AP \n","0.72% = street_light AP \n","8.82% = traffic_light AP \n","58.83% = train AP \n","6.00% = tree AP \n","36.11% = truck AP \n","20.44% = warning_column AP \n","58.51% = waste_container AP \n","mAP = 33.99%\n","Calculating mAP at 0.75 IoU threshold...\n","20.28% = bench AP \n","30.79% = bicycle AP \n","58.19% = bus AP \n","56.62% = bus_stop AP \n","36.03% = car AP \n","0.00% = crutch AP \n","18.61% = curb AP \n","24.98% = dog AP \n","44.96% = fire_hydrant AP \n","35.07% = motorcycle AP \n","3.54% = person AP \n","0.25% = pole AP \n","36.45% = spherical_roadblock AP \n","29.08% = stairs AP \n","70.62% = stop_sign AP \n","0.11% = street_light AP \n","4.26% = traffic_light AP \n","56.93% = train AP \n","4.66% = tree AP \n","33.06% = truck AP \n","17.01% = warning_column AP \n","55.10% = waste_container AP \n","mAP = 28.94%\n","Calculating mAP at 0.80 IoU threshold...\n","12.60% = bench AP \n","27.65% = bicycle AP \n","55.14% = bus AP \n","50.33% = bus_stop AP \n","31.58% = car AP \n","0.00% = crutch AP \n","13.14% = curb AP \n","18.62% = dog AP \n","41.23% = fire_hydrant AP \n","24.32% = motorcycle AP \n","1.65% = person AP \n","0.19% = pole AP \n","24.07% = spherical_roadblock AP \n","16.67% = stairs AP \n","65.47% = stop_sign AP \n","0.07% = street_light AP \n","0.64% = traffic_light AP \n","53.33% = train AP \n","2.11% = tree AP \n","25.21% = truck AP \n","9.48% = warning_column AP \n","46.88% = waste_container AP \n","mAP = 23.65%\n","Calculating mAP at 0.85 IoU threshold...\n","7.58% = bench AP \n","15.05% = bicycle AP \n","44.39% = bus AP \n","39.60% = bus_stop AP \n","25.88% = car AP \n","0.00% = crutch AP \n","5.40% = curb AP \n","12.12% = dog AP \n","32.63% = fire_hydrant AP \n","14.18% = motorcycle AP \n","0.38% = person AP \n","0.08% = pole AP \n","13.34% = spherical_roadblock AP \n","12.08% = stairs AP \n","51.70% = stop_sign AP \n","0.05% = street_light AP \n","0.47% = traffic_light AP \n","36.61% = train AP \n","0.69% = tree AP \n","19.08% = truck AP \n","3.80% = warning_column AP \n","34.56% = waste_container AP \n","mAP = 16.80%\n","Calculating mAP at 0.90 IoU threshold...\n","4.83% = bench AP \n","5.03% = bicycle AP \n","30.46% = bus AP \n","1.64% = bus_stop AP \n","13.17% = car AP \n","0.00% = crutch AP \n","2.01% = curb AP \n","4.10% = dog AP \n","16.32% = fire_hydrant AP \n","2.83% = motorcycle AP \n","0.14% = person AP \n","0.00% = pole AP \n","3.79% = spherical_roadblock AP \n","3.25% = stairs AP \n","29.88% = stop_sign AP \n","0.00% = street_light AP \n","0.00% = traffic_light AP \n","14.36% = train AP \n","0.18% = tree AP \n","8.80% = truck AP \n","0.33% = warning_column AP \n","21.09% = waste_container AP \n","mAP = 7.37%\n","Calculating mAP at 0.95 IoU threshold...\n","0.00% = bench AP \n","2.20% = bicycle AP \n","0.65% = bus AP \n","0.00% = bus_stop AP \n","1.53% = car AP \n","0.00% = crutch AP \n","0.00% = curb AP \n","0.36% = dog AP \n","0.53% = fire_hydrant AP \n","0.00% = motorcycle AP \n","0.00% = person AP \n","0.00% = pole AP \n","0.04% = spherical_roadblock AP \n","0.46% = stairs AP \n","4.30% = stop_sign AP \n","0.00% = street_light AP \n","0.00% = traffic_light AP \n","0.83% = train AP \n","0.13% = tree AP \n","0.78% = truck AP \n","0.00% = warning_column AP \n","3.09% = waste_container AP \n","mAP = 0.68%\n","\n","***mAP Results***\n","\n","Class\t\tAverage mAP @ 0.5:0.95\n","---------------------------------------\n","bench\t\t21.33%\n","bicycle\t\t29.65%\n","bus\t\t49.08%\n","bus_stop\t\t55.25%\n","car\t\t33.26%\n","crutch\t\t6.60%\n","curb\t\t17.45%\n","dog\t\t29.22%\n","fire_hydrant\t\t45.21%\n","motorcycle\t\t30.57%\n","person\t\t4.84%\n","pole\t\t2.55%\n","spherical_roadblock\t\t37.94%\n","stairs\t\t29.79%\n","stop_sign\t\t60.10%\n","street_light\t\t1.47%\n","traffic_light\t\t6.78%\n","train\t\t48.82%\n","tree\t\t5.22%\n","truck\t\t28.75%\n","warning_column\t\t18.93%\n","waste_container\t\t47.77%\n","\n","Overall\t\t27.75%\n"]}],"source":["%cd /content/drive/MyDrive/mAP\n","!python /content/drive/MyDrive/mAP/calculate_map_cartucho.py --labels=/content/drive/MyDrive/OOD/labelmap.txt"]},{"cell_type":"markdown","metadata":{"id":"R9HPoOBVKvxU"},"source":["The score reported at the end is your model's overall mAP score. Ideally, it should be above 50% (0.50). If it isn't, you can increase your model's accuracy by adding more images to your dataset. See my [dataset video](https://www.youtube.com/watch?v=v0ssiOY6cfg) for tips on how to capture good training images and improve accuracy."]},{"cell_type":"markdown","metadata":{"id":"5i40ve0SCLaE"},"source":["# 8.&nbsp;Deploy TensorFlow Lite Model"]},{"cell_type":"markdown","metadata":{"id":"phT8vvzriqQp"},"source":["Now that your custom model has been trained and converted to TFLite format, it's ready to be downloaded and deployed in an application! This section shows how to download the model and provides links to instructions for deploying it on the Raspberry Pi, your PC, or other edge devices."]},{"cell_type":"markdown","metadata":{"id":"zq3L2IoP4VHp"},"source":["## 8.1. Download TFLite model\n","\n","Run the two following cells to copy the labelmap files into the model folder, compress it into a zip folder, and then download it. The zip folder contains the `detect.tflite` model and `labelmap.txt` labelmap files that are needed to run the model in your application."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awZMQGVqMpVL"},"outputs":[],"source":["# Move labelmap and pipeline config files into TFLite model folder and zip it up\n","!cp /content/labelmap.txt /content/custom_model_lite\n","!cp /content/labelmap.pbtxt /content/custom_model_lite\n","!cp /content/models/mymodel/pipeline_file.config /content/custom_model_lite\n","\n","%cd /content\n","!zip -r custom_model_lite.zip custom_model_lite"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVPfAGbNPV56"},"outputs":[],"source":["from google.colab import files\n","\n","files.download('/content/custom_model_lite.zip')"]},{"cell_type":"markdown","metadata":{"id":"9Kb3ZBsMq95l"},"source":["The `custom_model_lite.zip` file containing the model will download into your Downloads folder. It's ready to be deployed on your device!"]},{"cell_type":"markdown","metadata":{"id":"GSJ2wgGCixy2"},"source":["## 8.2. Deploy model\n","TensorFlow Lite models can run on a wide variety of hardware, including PCs, embedded systems, and phones. This section provides instructions showing how to deploy your TFLite model on various devices.\n","\n","### 8.2.1. Deploy on Raspberry Pi\n","TFLite models are great for running on the Raspberry Pi, because they require less processing power than regular TensorFlow vision models. The Pi can run TFLite models in near real-time.\n","\n","To run your new model on the Raspberry Pi, you'll have to install TensorFlow Lite and prepare a Python environment for your application. I provide step-by-step instructions on how to set up TFLite on the Pi in my video, [How To Run TensorFlow Lite on Raspberry Pi for Object Detection](https://youtu.be/aimSGOAUI8Y).\n","\n","[![Link to my YouTube video!](https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/doc/YouTube_video1.JPG)](https://www.youtube.com/watch?v=aimSGOAUI8Y)\n","\n","Once you've completed all the steps in the video, move the `custom_model_lite.zip` file downloaded from this Colab session over to your Raspberry Pi into the `~/tflite1` folder. Move into the folder and unzip it by issuing:\n","\n","```\n","cd ~/tflite1\n","unzip custom_model_lite.zip\n","```\n","\n","Then, run the image, video, or webcam TFLite detection program with the `--modeldir=fine_tuned_model_lite` argument. For example, to run the webcam detection program, issue:\n","\n","```\n","python TFLite_detection_webcam.py --modeldir=custom_model_lite\n","```\n","\n","A window will appear showing a live feed from your webcam with boxes drawn around detected objects in each frame.\n","\n","### 8.2.2. Deploy on Windows, Linux, or macOS\n","Follow the instructions linked below to quickly set up your Windows, Linux, or macOS computer to run TFLite models. It only takes a few minutes! Running a model on your PC is good for quickly testing your model with a webcam. However, keep in mind that the TFLite Runtime is optimized for lower-power processors, and it won't utilize the full capability of your PC's processor.\n","\n","Here are links to the deployment guides for Windows, Linux, and macOS:\n","* [How to Run TensorFlow Lite Models on Windows](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/deploy_guides/Windows_TFLite_Guide.md)\n","* *link to Linux guide to be added (but really it's the same as Raspberry Pi)*\n","* *link to macOS guide to be added*\n","\n","### 8.2.3. Deploy on other Linux-based edge devices\n","Instructions to be added! 🐧\n","\n","### 8.2.4. Deploy on Android\n","Instructions to be added! 🤖\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WoptFnAhCSrR"},"source":["# 9.&nbsp;(Optional) Post-Training Quantization"]},{"cell_type":"markdown","metadata":{"id":"I54paUm8dUCr"},"source":["Want to make your TFLite model run even faster? Quantize it! Quantizating a model converts its weights from 32-bit floating-point values to 8-bit integer values. This allows the quantized model to run faster and occupy less memory without too much reduction in accuracy.\n","\n","> Note: If you observe an obvious decrease in detection accuracy when quantizing your model with TF2, I recommend using TensorFlow 1 to quantize your model instead. TF1 supports quantization-aware training, which helps improve the accuracy of quantized models. The ssd-mobilenet-v2-quantized model from the TF1 Model Zoo has fast and accurate performance when trained with a custom dataset. Visit my [TFLite v1 Colab notebook](https://colab.research.google.com/github/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Train_TFLite1_Object_Detection_Model.ipynb) for step-by-step instructions on how to train and quantize a model with TensorFlow 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFTXZeZsHACI"},"outputs":[],"source":["!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"XfgCvR0eQgVXk1Wu8DbK\")\n","project = rf.workspace(\"fpn\").project(\"ood-pbnro\")\n","dataset = project.version(1).download(\"voc\")"]},{"cell_type":"markdown","metadata":{"id":"VTyqlXFTJ0Uv"},"source":["## 9.1. Quantize model\n","We'll use the \"TFLiteConverter\" module to perform [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) on the model. To quantize the model, we need to provide a representative dataset, which is a set of images that represent what the model will see when deployed in the field. First, we'll create a list of images to include in the representative dataset (we'll just use the images in the `train` folder).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-j_X1jnNTdu"},"outputs":[],"source":["# Import packages\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import glob\n","import random\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSNZtfj_k3NP"},"outputs":[],"source":["# Get list of all images in train directory\n","import glob\n","image_path = '/content/drive/MyDrive/OOD-1/train'\n","\n","jpg_file_list = glob.glob(image_path + '/*.jpg')\n","JPG_file_list = glob.glob(image_path + '/*.JPG')\n","png_file_list = glob.glob(image_path + '/*.png')\n","bmp_file_list = glob.glob(image_path + '/*.bmp')\n","\n","quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list"]},{"cell_type":"markdown","metadata":{"id":"cqbH1VlEgiuy"},"source":["Next, we'll define a function to yield images from our representative dataset. Refer to [TensorFlow's sample quantization code](https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb#scrollTo=kRDabW_u1wnv) to get a better understanding of what this is doing!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLiPcVaNMn8E"},"outputs":[],"source":["# Set up variables for running user's model\n","PATH_TO_MODEL='/content/drive/MyDrive/yolo-v8-n-s/yolov5n-100-epochs/yolov5n-fp16.tflite'   # Path to .tflite model file\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORzx0XRErSLV"},"outputs":[],"source":["# A generator that provides a representative dataset\n","# Code modified from https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\n","\n","# First, get input details for model so we know how to preprocess images\n","interpreter = Interpreter(model_path=PATH_TO_MODEL) # PATH_TO_MODEL is defined in Step 7 above\n","interpreter.allocate_tensors()\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","height = input_details[0]['shape'][1]\n","width = input_details[0]['shape'][2]\n","\n","import random\n","\n","def representative_data_gen():\n","  dataset_list = quant_image_list\n","  quant_num = 300\n","  for i in range(quant_num):\n","    pick_me = random.choice(dataset_list)\n","    image = tf.io.read_file(pick_me)\n","\n","    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n","      image = tf.io.decode_jpeg(image, channels=3)\n","    elif pick_me.endswith('.png'):\n","      image = tf.io.decode_png(image, channels=3)\n","    elif pick_me.endswith('.bmp'):\n","      image = tf.io.decode_bmp(image, channels=3)\n","\n","    image = tf.image.resize(image, [width, height])  # TO DO: Replace 300s with an automatic way of reading network input size\n","    image = tf.cast(image / 255., tf.float32)\n","    image = tf.expand_dims(image, 0)\n","    yield [image]"]},{"cell_type":"markdown","metadata":{"id":"wqtu98mzebEj"},"source":["Finally, we'll initialize the TFLiteConverter module, point it at the TFLite graph we generated in Step 6, and provide it with the representative dataset generator function we created in the previous code block. We'll configure the converter to quantize the model's weight values to INT8 format."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326108,"status":"ok","timestamp":1688429529813,"user":{"displayName":"Ahmed Boussihmed","userId":"16936910001719053027"},"user_tz":420},"id":"Ox0bGDWds_Ce","outputId":"4767fb6c-e6f0-4c0a-a75b-b24d8f11f707"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Importing a function (__inference_pruned_7112) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"]}],"source":["import tensorflow as tf\n","# Initialize converter module\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/MyDrive/yolo-v8-n-s/yolov5n-100-epochs/yolov5n_saved_model')\n","\n","# This enables quantization\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","# This sets the representative dataset for quantization\n","converter.representative_dataset = representative_data_gen\n","# This ensures that if any ops can't be quantized, the converter throws an error\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n","converter.target_spec.supported_types = [tf.int8]\n","# These set the input tensors to uint8 and output tensors to float32\n","converter.inference_input_type = tf.uint8\n","converter.inference_output_type = tf.float32\n","tflite_model = converter.convert()\n","\n","with open('/content/detect_quant.tflite', 'wb') as f:\n","  f.write(tflite_model)"]},{"cell_type":"markdown","metadata":{"id":"dYVVlv5QUUZF"},"source":["## 9.2. Test quantized model\n","The model has been quantized and exported as `detect_quant.tflite`. Let's test it out! We'll re-use the function from Section 7 for running the model on test images and display the results, except this time we'll point it at the quantized model.\n","\n","Click Play on the code block below to test the `detect_quant.tflite` model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":609,"output_embedded_package_id":"1cZ2oNTik5-g_XZA5ApT3Vwcq8PsbOk0p"},"id":"6OoirJuOtdOG","outputId":"2820b706-a66d-42f4-82ff-b2b0a2ff9bfe"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Set up parameters for inferencing function (using detect_quant.tflite instead of detect.tflite)\n","PATH_TO_IMAGES='/content/drive/MyDrive/OOD-1/test'   #Path to test images folder\n","PATH_TO_MODEL='/content/drive/MyDrive/ssd-mobilenet-v2-fpn-lite-640x640/custom_model_lite/ssd-mobilenet-v2-fpn-lite-640x640.tflite'   #Path to .tflite model file\n","PATH_TO_LABELS='/content/drive/MyDrive/OOD-1/labelmap.txt'   #Path to labelmap.txt file\n","min_conf_threshold=0.5   #Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n","images_to_test = 10   #Number of images to run detection on\n","\n","# Run inferencing function!\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"]},{"cell_type":"markdown","metadata":{"id":"cKo7ZtfOyoxG"},"source":["If your quantized model isn't performing very well, try using my TensorFlow Lite 1 notebook *(link to be added)* to train a SSD-MobileNet model with your dataset. In my experience, the `ssd-mobilenet-v2-quantized` model from the [TF1 Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) has the best quantized performance out of any other TensorFlow Lite model.\n","\n","TFLite models created with TensorFlow 1 are still compatible with the TensorFlow Lite 2 runtime, so your TFLite 1 model will still work with my [TensorFlow setup guide for the Raspberry Pi](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md)."]},{"cell_type":"markdown","metadata":{"id":"vWdVxs6LUjbR"},"source":["## 9.3 Calculate quantized model mAP\n","\n","Let's calculate the quantize model's mAP using the calculator tool we set up in Step 7.2. We just need to perform inference with our quantized model (`detect_quant.tflite`) to get a new set of detection results.\n","\n","Run the following block to run inference on the test images and save the detection results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMaumV-11Et0"},"outputs":[],"source":["# Need to remove existing detection results first\n","!rm /content/mAP/input/detection-results/*\n","\n","# Set up variables for running inference, this time to get detection results saved as .txt files\n","PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n","PATH_TO_MODEL='/content/custom_model_lite/detect_quant.tflite'   # Path to quantized .tflite model file\n","PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n","PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n","min_conf_threshold=0.1   # Confidence threshold\n","\n","# Use all the images in the test folder\n","image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n","images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n","\n","# Tell function to just save results and not display images\n","txt_only = True\n","\n","# Run inferencing function!\n","print('Starting inference on %d images...' % images_to_test)\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n","print('Finished inferencing!')"]},{"cell_type":"markdown","metadata":{"id":"QgcmdLQf1Et1"},"source":["Now we can run the mAP calculation script to determine our quantized model's mAP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIRNp0Af1Et1"},"outputs":[],"source":["cd /content/mAP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TDgMBw_1Et1"},"outputs":[],"source":["!python calculate_map_cartucho.py --labels=/content/labelmap.txt"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["4VAvZo8qE4u5","sxb8_h-QFErO","eydREUsMGUUR","eGEUZYAMEZ6f","-19zML6oEO7l","kPg8oMnQDYKl","RDQrtQhvC3oG","5i40ve0SCLaE","WoptFnAhCSrR","5VI_Gh5dCd7w"],"provenance":[{"file_id":"https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Train_TFLite2_Object_Detction_Model.ipynb","timestamp":1686058231045}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}